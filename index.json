
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    [{"authors":null,"categories":[""],"content":" Why SSIA\nWhat OSS LICENSE https://github.com/github/opensource.guide\nhttps://opensource.guide/\nhttps://choosealicense.com/\n-\u0026gt; GitHub社がライセンス選定指針のサイトを作成。 GitHub上のライセンスは、リポジトリの利用条件を明確にし、他の開発者がそのコードをどのように使用、変更、配布できるかを定義するために重要。 ライセンスを適切に設定することで、著作権侵害のリスクを軽減し、 オープンソースコミュニティの一員 としてのルールを守ることができる。 メインオープンソースライセンス MIT License 特徴: 非常に寛容なライセンス で、ほぼ無制限にコードを使用、コピー、変更、合併、公開、配布、サブライセンス、販売することができる。 条件: ただし、 元の著作権表示 と ライセンス通知 を全てのコピーや著作物に含める必要がある。 GNU GPL (General Public License) 特徴: コピーや配布、変更の自由を保障しつつも、変更したバージョンも同じライセンスで配布することを求めている。 条件: 変更したコードを配布する際に、元の著作権表示、ライセンス通知、およびソースコードを提供する義務がある。 Apache License 2.0 特徴: 商用利用、特許権の明示的な許諾、改変後のコードの配布を許可している。 条件: 著作権表示、ライセンス通知、変更点を明示することを求めている。 BSD License (3-clause BSD License) 特徴: MITライセンスと似ていますが、追加の条件として、広告材料に貢献者の名前を使わないことを求めている。 条件: 著作権表示、ライセンス通知を含めること。 その他 主要ライセンス以外では以下の記事がより詳細 cf. たくさんあるオープンソースライセンスのそれぞれの特徴のまとめ coliss 利用方法（MIT） MITライセンスのリポジトリをクローンした場合\n使ったり変更したり: MITライセンスのリポジトリをクローン、そのコードを自由に使用、変更できる。 e.g. 自分のプロジェクトにそのコードを取り込む場合や、改良を加えて再配布することも可能。 配ったり: 変更を加えたコードを再配布する際には、元の著作権表示とライセンス通知を含める必要がある。 リポジトリにMITライセンスを適用\nライセンスファイルの作成 $ touch LICENSE MIT License Copyright (c) [year] [fullname] Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u0026#34;Software\u0026#34;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \u0026#34;AS IS\u0026#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. e.g. https://github.com/hubotio/hubot/blob/main/LICENSE.md README.mdにライセンス情報を追加 ## License This project is licensed under the MIT License - see the [LICENSE](license-link) file for details. cf. https://docs.github.com/ja/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/licensing-a-repository Comunication 心得 GitHubでのコメントは、オープンソース（OSS）のプロジェクト管理やチーム内の協力を円滑に進めるために非常に重要。 詰まるところプロジェクトの成功に欠かせない要素だとも思う。 丁寧で建設的なコメントは、コミュニケーションを円滑にし、プロジェクトの質向上の助けになると信じ、敬意を持ち、具体的で明確なフィードバックを心がけることで、良好なコラボレーション環境を築いていきたい。ref\nGitHubでのコメントの際に注意すべきポイントや、適切なマナーについてや、マネしたいと思う事、素晴らしいと思ったことを振り替えれるように箇条書きしておきたい。（いつかの自分の為にも）\nマナー・エチケット編 社会人ならば \u0026#34;こんなこと\u0026#34;、と軽視しがちな当たり前を言語化を目指したい。 敬意を持つ: 他の貢献者に対して敬意を持ってコメントをする。礼儀正しい言葉遣いを心がけ、攻撃的な言葉やトーンは避ける。 建設的なフィードバック: 批判する際には、具体的な問題点と改善提案を含めるようにする。単なる否定的なコメントは避け、建設的なアプローチを心掛けたい。 感謝の気持ちを表す: 貢献や助けてくれたことに対して感謝の意を示すことは、コミュニティのモチベーションを高めるため。善意ある行動は当たり前ではなく、感謝すべき点である。 具体的で明確: コメントは具体的で明確に。曖昧な表現は避け、__誰でも理解できる__ようにする。 関連性のある情報を提供: 問題や提案に関連する情報を提供。コードの一部やリンクを含めると、理解しやすく優しい世界。 質問を明確にする: 問題点や疑問点を明確にし、具体的な質問をすることで、迅速で適切な回答を得られる。 思想編 思想的なところは、どちらかというと付加価値を獲るためのものと考える。 協力的な姿勢: コミュニティは協力によって成り立っている事は間違いない。問題解決や改善に向けて協力する姿勢を持ちたい。 透明性: 意見や決定は透明性を持って行いたい。背後での議論は避け、オープンな場でのコミュニケーションを心がけたい。 学習と成長: 自分の意見が常に正しいとは限らない。全ては 学習の機会 と捉え、他者の意見にもしっかりと傾聴したい。 アプローチとテクニック編 適切なスレッド: 適切なスレッドやプルリクエスト、イシューでコメントを行う。関係のない場所でのコメントは避ける。\nFMTの活用: GitHubのマークダウン機能を活用し、コードブロックやリスト、リンクを使ってコメントを見やすく整理。\nTMPの活用: 特定の形式でコメントを求める場合、テンプレートを用意しておくと効率的。\n語学力（適切な文法）: 会話ならばボディランゲージや表情が助けになる。しかし、ドキュメントのみというとそうはいかない。適切な言葉遣い、読解力等は避けて通れない。（特にOSSに参加するなら尚のこと…翻訳ばかりに頼り切らず頑張らねば！きっと楽しい）\nSuggestion\nコードの一部分を改変依頼を出したい時に見やすく分かり易く依頼を投げることができるサジェスション機能。ref\nHide Hide comment “Choose a reason”\n・Spam スパム 👎 ・Abuse 侵害・雑言 🤬 ・Off topic スレ違い 👋 ・Outdated 古い・期限切れ 😇 ・Resolved 解決済み 💮 闇無もに非表示せず、あくまで見やすいを心掛けつつも、意図が損なわないものにと配慮が必要。ref\nPending 問題 (Pending) マークがついていると記述者は見えているが、他の人には見えない状態なので回避したい。ref 解決 FileChanged \u0026gt; Review changes \u0026gt; Submit review 課題 基本的には、[Conversation]タブから返信していれば、pending状態にはならないよう。 一度でも[FileChanged]タブから[Start a review]ボタンでコメント返信したあとに、 [Conversation]タブ上から返信するとすべてpending状態になってしまうよう。 Ref. GitHubが公開してるオープンソースガイドライン、まとめてみた。 Zenn ライセンスをつけないとどうなるの？ Qiita GitHubでライセンスを設定する Qiita GitHubライセンス修正方法 note 製品のライセンス体系、機能比較 XLSOFT Githubによる、オープンソースライセンスの選び方 GitHub でのコミュニケーション GitHub Docs 混乱を生じるコメントを管理する GitHub Docs GitHub の Hide comment “Choose a reason” HatenaBlog （コミュニケーション）コメントを「する」技術を考える Zenn すぐに始められる！GitHubコミュニケーションでチームの情報共有コストを下げよう Money Forward github-comment で PR にコメントをして CI の結果を分かりやすくする Zenn GitHub コードレビューでサジェスション機能を使って差分を提案しよう アルパカログ 【GitHub】プルリクにコメント返信した\u0026#34;つもり\u0026#34;になってた話 Qiita ","date":1732158368,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732158368,"objectID":"047775cdeb469d87337609a24f532cd4","permalink":"http://localhost:1313/todayilearned/post/github_wana_be_join_oss/","publishdate":"2024-11-21T12:06:08+09:00","relpermalink":"/post/github_wana_be_join_oss/","section":"post","summary":"📍 ","tags":["GitHub","KnowHow","OSS"],"title":"GitHub Wana be join OSS","type":"post"},{"authors":null,"categories":[""],"content":" Why SSIA\nWhat What is Kubernetes (K8s) https://kubernetes.io/ | ja | doc\nKubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative c- onfiguration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available. ref\nKubernetes, also known as k8s, can automate many of the manual processes involved in deploying, managing, and scaling containerized applications.\nSo what does Kubernetes actually provide?\nService discovery and load balancing - Kubernetes can expose a container using their domain name or using their own IP address. If traffic to a container is high, Kubernetes is able to load balance and distribute the network traffic so that the deployment is stable. Storage orchestration - Kubernetes allows you to automatically mount a storage system of your choice, such as local storages, public cloud providers, and more. Automated rollouts and rollbacks - You can describe the desired state for your deployed containers using Kubernetes, and it can change the actual state to the desired state at a controlled rate. For example, you can automate Kubernetes to create new containers for your deployment, remove existing containers and adopt all their resources to the new container. Automatic bin packing - You provide Kubernetes with a cluster of nodes that it can use to run containerized tasks. You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit containers onto your nodes to make the best use of your resources. Self-healing - Kubernetes restarts containers that fail, replaces containers, kills containers that don’t respond to your user-defined health check, and doesn’t advertise them to clients until they are ready to serve. Secret and configuration management - Kubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens, and SSH keys. You can deploy and update secrets and application configuration without rebuilding your container images, and without exposing secrets in your stack configuration. Hands-on Tutorial\nPlease check the following tutorial made by the Kubernetes official site. It provides a walkthrough of the basics of the Kubernetes cluster orchestration system. Each module contains some background information on major Kubernetes features and concepts with interactive tutorials. You will be albe to have basic ideas by managing a simple cluster and its containerized applications for yourself. Learn Kubernetes Basics Kubernetes Terminology and Concepts Before starting to use Kubernetes, we need to know some basic terminology and concepts in Kubernetes. Pod Pods are the smallest deployable units of computing that you can create and manage in Kubernetes. Each Pod is meant to run a single instance of a given application, like one API server. Each pod has its own yaml file which is the definition of this pod. A Pod can have one or more containers, with shared storage and network resources, and a specification for how to run the containers. Worker node The worker node(s) host the Pods that are the components of the application workload. Every node has three components: kubelet,kube-proxy and Container Runtime. kubelet - The primary “node agent” that runs on each node.It ensures all pods of the node are running and healthy. kube-proxy - A network proxy that runs on each node in your cluster which maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster. Container runtime - The software that is responsible for running containers. Control Plane The control plane manages the worker nodes and the Pods in the cluster. kube-apiserver - The API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane. etcd - Consistent and highly-available key value store used as Kubernetes’ backing store for all cluster data. kube-scheduler - The scheduler monitors the newly created Pods that have not yet been specified on which Node to run, and coordinates the most suitable node to run the pod according to the resource regulations and hardware constraints on each node. kube-controller-manager - Responsible for managing and running the components of the Kubernetes controller. Simply put, the controller is the process in Kubernetes that is responsible for monitoring the status of the Cluster, such as Node Controller, Replication Controller. Cluster A Kubernetes cluster is made up of one control plane and several worker nodes. Networking Kubernetes defines a network model that helps provide simplicity and consistency across a range of networking environments and network implementations. The Kubernetes network model specifies: Every pod gets its own IP address Containers within a pod share the pod IP address and can communicate freely with …","date":1731904173,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1731904173,"objectID":"3d308ac0ceb409410ed6e2e6eb122360","permalink":"http://localhost:1313/todayilearned/post/kubernetes_catchup/","publishdate":"2024-11-18T13:29:33+09:00","relpermalink":"/post/kubernetes_catchup/","section":"post","summary":"⚓️ About container orchestration tecnology.","tags":["Kubernetes","Docker","CUE"],"title":"Kubernetes Catchup","type":"post"},{"authors":null,"categories":["Bookread"],"content":" Why SSIA\nWhat Udemy講座 BigQuery で学ぶ非エンジニアのための SQL データ分析入門 Udemy 当時の所感: 2021-09-10 はじめに この記事は、Udemyにて学習をした際のメモです。 コース受講したい場合は、ページ最下部に参考リンクとして掲載いたします。 学習メモ 次章からは、完全個人メモです。（後に自分で見返すようになっております） どんな考え方（ロジック）でクエリを書き上げたのか、どこで躓いたのか、 後に俯瞰したいためにお恥ずかしながら演習問題で間違えてしまったクエリはそのまま（コメントアウト）記載してます。 悪しからず... おわりに はじめて BigQueryに触れましたが、 標準SQLの奥の深さ、データを抽出することの難しさに新たな課題感を感じております。 ですが、本講座を通して、BigQueryの仕様や、SQLの基本的な文法は体得したのではないのかな？と思います。 （自分でも手が動くのに驚きを隠せませんでした。） 前半は基礎文法と地道ではありましたが、後半はパズルゲームを解く感覚に近くとても楽しんで取り組むことができました。 SECTION2~3：歴史と基本設定 データベース\nOSS （Open Source Softwear） MySQL、PostgreSQL、SQLitte …etc. PaaS （） Google BigQuery、Amazon Web Server、TreasureData …etc. 2種類のサービス形態がある。 平たくまとめるとお金を取るか、取らないか。無料で利用できるのがOSS。 BigQueryは無料枠もあるが、処理に応じて従量課金型。 SQLとは\n-- {テーブル名}から、{カラム名}に記載の列を取得してきて！って意味 SELECT {カラム名} FROM {テーブル名}; SQL生みの親 SQL（Structured Query Language：構造化された検索言語）の略で、古くはIBM社が1970年台に開発していたデータベース管理システムの制御用の言語、SEQUEL（Structured English Query Language）が元になっている。\ncf. The Relational Model Content based on Chapter 3 SlideToDoc\nSQLは、関係データベース管理システム において、データの操作や定義を行うためのデータベース言語、ドメイン固有言語である。プログラミングにおいてデータベースへのアクセスのために、他のプログラミング言語と併用される。\ncf. SQL Wekipedia\nGoogle BigQueryとは？\n-\u0026gt; ざっくり、Googleが提供するPaaS（Platform as a Service）のこと。 BigQuery（ビッグクエリ）は、ペタバイト単位のデータに対するスケーラブルな分析を可能にする、フルマネージドのサーバーレスのデータウェアハウスである。ANSI SQLを使用したクエリをサポートするPlatform as a Service（PaaS）としてGoogle Cloud Platformにより提供されている。また、機械学習の機能も組み込まれている。BigQueryは2010年5月に発表され、2011年11月に一般提供（GA）となった。\ncf. BigQuery Wikipedia\n適切なハードウェアとインフラストラクチャを用意せずに大規模なデータセットを保存し、それに対してクエリを実行すると、時間と費用がかかってしまいます。エンタープライズ データ ウェアハウスである Google BigQuery は、Google のインフラストラクチャの処理能力を活用して SQL クエリを超高速で実行し、こうした問題を解決します。データを BigQuery に読み込んだら、後の処理は Google にお任せください。また、データの表示やクエリの実行権限を自分以外のユーザーに付与できるため、業務上の必要に応じてプロジェクトや自分のデータに対するアクセスを制御することも可能です。\ncf. BigQuery とは GoogleCloud\nBigQuery環境構築\nBigQueryにアクセス https://cloud.google.com/bigquery/ 完成イメージとしては以下のような図。 外枠にプロジェクトがあり、その中にデータセットがあり、更にその中に各テーブル（Excelでいうタブ）がある。 入れ子（ネスト）構造。 プロジェクトを作成 データセットを作成 テーブルを作成 SECTION4 ：基本文法 記述順序\n1. SELECT ：取得する列の指定 2.（集計関数） 3. FROM ：取得するテーブルの指定 4. JOIN ：テーブルの結合の処理 5. ON / USING ：結合に利用するキー指定 6. WHERE ：絞り込み条件の指定 7. GROUP BY ：グループ化項目の指定 8. HAVING ：グループ化された集計結果に対して絞り込み条件の指定 9. ORDER BY ：並び替え条件 10. LIMIT ：表示する行数の指定 実行順序\n1. FROM 2. ON / USING 3. JOIN 4. WHERE 5. GROUP BY 6. (集計関数) 7. HAVING 8. SELECT 9. ORDER BY 10. LIMIT SELECT句 - かラム指定\n-- shop_purchasesテーブルからpurchase_id,user_id,dateを取り出してください。 SELECT purchase_id, user_id, date FROM bq_sample.shop_purchases ; -- | |purchase_id|user_id|date | -- |1| 22| 956425|2018-01-05| -- |2| 388| 937162|2018-04-30| DISTINCT句 - 重複除外\n-- shop_purchasesを利用して、固有のユーザーが何人いるかを取得。 SELECT DISTINCT user_id FROM bq_sample.shop_purchases ; -- | |user_id| -- |1| 956425| -- |2| 937162| EXCEPT() - かラム除外\n-- shop_purchasesテーブルから全カラムを取得。 SELECT * FROM bq_sample.shop_purchases; --70.1KB -- shop_purchasesテーブルからshop_id、product_idを除くカラムを取得。 -- ie. 抽出時に除外したいカラム名を記載。 SELECT * EXCEPT(shop_id, product_id) FROM bq_sample.shop_purchases; --50.1KB ORDERBY句 - ソート機能 e.g.\nORDER BY {カラム番号,カラム} (ASC:昇順 DESC:降順） … [1]indexに注意 -- shop_purchasesテーブルから、user_id、quantityを取得し、 -- quantityの大きい順に並び替える。 SELECT user_id, quantity FROM bq_sample.shop_purchases ORDER BY quantity DESC -- ORDER BY 2 DESC; ; -- | | user_id| quantity| -- |1| 971235| 5| -- |2| 992842| 5| -- shop_purchasesテーブルの全列を取得し、dateの古い順(日付順)に並べる。 -- もし、同じ日に複数行ある場合、sales_amountの大きい順に並べる。 -- さらに、sales_amountも同額であった場合には、quantityの大きい順に並べる。 SELECT * FROM bq_sample.shop_purchases --日付順(昇順) \u0026amp; 同日の場合売上高降順 \u0026amp; 数量降順 ORDER BY date, sales_amount DESC, quantity DESC ; -- [ASC] [DESC] [DESC] -- | |purchase_id|user_id|date |shop_id|product_id|quantity|sales_amount| -- |1| 6| 954830|2018-01-01| 1| 17| 3| 15488| -- |2| 1| 733995|2018-01-01| 2| 10| 1| 12775| -- shop_purchasesテーブルの全列を取得し、dateの新しい順（日付逆順）に並べる。 -- もし、同日複数のレコードがある場合には、sales_amountの小さい順に並べる。 -- 但、並べ替えには列の順序番号を利用すること。 SELECT * FROM bq_sample.shop_purchases ORDER BY 3 DESC, 7 ASC ; -- [DESC] [ASC] -- | |purchase_id|user_id|date |shop_id|product_id|quantity|sales_amount| -- |1| 1281| 873505|2018-12-31| 2| 7| 1| 7527| -- |2| 1280| 840135|2018-12-31| 1| 11| 2| 7527| -- |3| 1282|1118265|2018-12-31| 1| 15| 4| 16000| LIMIT句 - レコード制限\n※ 表示レコード数を制限するだけで、後ろの計算は全行行われている。 そのため発生料金に対して節約できるわけではない。 ex.【4.5 演習問題1 (0:55)】\nshop_purchasesテーブルから、全カラムを5行だけ取得してください。\nSELECT * FROM bq_sample.shop_purchases LIMIT 5; ex.【4.5 演習問題2 (2:23)】\nshop_purchasesテーブルから、date大きい順（＝新しい順）に、全カラムを10行だけ取得してください。\nSELECT * FROM `prj-test3.bq_sample.shop_purchases` ORDER BY date DESC LIMIT 10; OFFSET e.g.\nLIMIT [取得したい行数] OFFSET [取得をずらす行数]; ※ ORDERBY を使用前提 ex.【4.5 演習問題3 (:)】取得制限\nshop_purchasesテーブルから、purchase_id、sales_amountを取得。その際sales_amont降順に並べた上で11位から5レコードだけを取得。\nSELECT purchase_id, sales_amount FROM `prj-test3.bq_sample.shop_purchases` ORDER BY sales_amount DESC LIMIT 5 OFFSET 10; 行 purchase_id sales_amont 1 233 73000 2 286 72568 ※ 上位10行から、上位5行（＝5位）まで取得している | WHERE句 - 絞り込み条件 e.g.\nshop_purchasesテーブルからquantity列の３より大きい行を取得。\nSELECT * FROM \u0026#39;bq_sample.shop_purchases\u0026#39; WHERE quantity \u0026gt; 3; ※ WHERE句はFROM句の直後に記述する。 ※ 絞り込みをして …","date":1729869725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1729869725,"objectID":"acd5e465d1954f0beb2763f4de031660","permalink":"http://localhost:1313/todayilearned/post/bigquery_data_analyize_catchup/","publishdate":"2024-10-26T00:22:05+09:00","relpermalink":"/post/bigquery_data_analyize_catchup/","section":"post","summary":"🔍 Re-summarizeing the trajectory I learned as an introductory.","tags":["BigQuery","SQL","Data Analytics","HandsOn","Udemy"],"title":"BigQuery Data Analyize Catchup","type":"post"},{"authors":null,"categories":[""],"content":" Why SSIA\nWhat https://wezfurlong.org/wezterm/ | dw (macOS) | overview ~/ | (XDGベースのディレクトリ構造を利用) └ .config/ └ wezterm/ └ wezterm.lua \u0026lt;-推奨！ └ .wezterm.lua \u0026lt;-互換性のためにサポートされている場所 i.e. ベストプラクティスは、~/.config/wezterm/wezterm.lua $HOME 直下に配置されているwezterm.luaは不可視にしてもしなくてもOK UXの観点で、ファイル名を.(ドット)で始めると、UNIX系システムではファイルが不可視（隠しファイル）として扱われる。 -- Pull in the wezterm API local wezterm = require \u0026#39;wezterm\u0026#39; -- This table will hold the configuration. local config = {} -- In newer versions of wezterm, use the config_builder which will -- help provide clearer error messages if wezterm.config_builder then config = wezterm.config_builder() end -- Apply your config choices here -- Notification when config is reloaded wezterm.on(\u0026#39;window-config-reloaded\u0026#39;, function(window, pane) wezterm.log_info \u0026#39;the config was reloaded for this window!\u0026#39; end) -- Alt + 矢印キーをVimに送信する config.keys = { { key = \u0026#34;DownArrow\u0026#34;, mods = \u0026#34;ALT\u0026#34;, action = wezterm.action.SendKey { key = \u0026#34;DownArrow\u0026#34;, mods = \u0026#34;ALT\u0026#34; } }, { key = \u0026#34;UpArrow\u0026#34;, mods = \u0026#34;ALT\u0026#34;, action = wezterm.action.SendKey { key = \u0026#34;UpArrow\u0026#34;, mods = \u0026#34;ALT\u0026#34; } }, } -- フルスクリーン切り替えやペイン作成 config.keys = { { key = \u0026#39;t\u0026#39;, mods = \u0026#39;SHIFT|CTRL\u0026#39;, action = wezterm.action.SpawnTab \u0026#39;CurrentPaneDomain\u0026#39;, }, { key = \u0026#39;d\u0026#39;, mods = \u0026#39;SHIFT|CTRL\u0026#39;, action = wezterm.action.SplitHorizontal { domain = \u0026#39;CurrentPaneDomain\u0026#39; }, }, } -- Color scheme config.color_scheme = \u0026#39;Builtin Dark\u0026#39; -- Background opacity and blur config.window_background_opacity = 0.85 config.macos_window_background_blur = 15 -- Font config.font = wezterm.font(\u0026#34;HackGen\u0026#34;, {weight=\u0026#34;Medium\u0026#34;, stretch=\u0026#34;Normal\u0026#34;, style=\u0026#34;Normal\u0026#34;}) config.font_size = 14 -- Finally, return the config return config WezTerm Setup HandsOn WezTerm インストール（CLI） $ brew install --cask wezterm $ wezterm --version i.e. wezterm コマンドを利用できるようにする方法（以下いずれか） 環境変数（PATH）設定 $ echo export PATH=\u0026#34;/Applications/WezTerm.app/Contents/MacOS:$PATH\u0026#34; \u0026gt;\u0026gt; ~/.zshrc $ source ~/.zshrc シンボリックリンク設定 $ sudo ln -s /Applications/WezTerm.app/Contents/MacOS/wezterm /usr/local/bin/wezterm WezTerm.app がアプリケーションフォルダに存在する場合、（WebUIでダウンロードした場合） -\u0026gt; /Applications/WezTerm.app/Contents/MacOS/WezTerm にあるWezTermのバイナリのシンボリックリンクを作成。 設定ファイルの配置場所の確認 $ ls -l ~/.wezterm.lua 設定ファイルの構文エラーの確認 $ wezterm start --verbose 再起動 $ wezterm cli reload Ref. 最高のターミナル環境を手に入れろ！WezTermに入門してみた。 DevelopersIO 君もイカしたTerminal環境で開発してみなイカ？(wezterm, zsh, starship, neovim) Zenn ","date":1729734032,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1729734032,"objectID":"55be86c38e349255ab1e39fc87b0a613","permalink":"http://localhost:1313/todayilearned/post/wezterm_setup/","publishdate":"2024-10-24T10:40:32+09:00","relpermalink":"/post/wezterm_setup/","section":"post","summary":"🟣 Would like to use the most powerful terminal that rumored on the street.","tags":["WezTerm","Lua","Terminal"],"title":"WezTerm Setup","type":"post"},{"authors":null,"categories":[""],"content":" Why SSIA\nWhat https://github.com/topics/google-sheets https://github.com/topics/google-apps-script Google Spread Sheets (以下:GSS) [GSS] 一括翻訳 googletranslate() ★ =arrayformula(trim(transpose(split(googletranslate(textjoin(\u0026#34;. \u0026#34;, 1, $B$2:$B$1000), \u0026#34;en\u0026#34;, \u0026#34;ja\u0026#34;), \u0026#34;。 \u0026#34;, true)))) No Phrases and vocabulary Meaning in japanese 1 excess permissions ★ 2 inventory confirmation deadline 在庫確認期限 3 escrow payment methods エスクロー支払い方法 ref cf. How can I use ARRAYFORMULA (or something similar) with GOOGLETRANSLATE? StackExchange [GSS] 他シートから取得 filter() ★ =iferror(filter(\u0026#39;ログ用Create文(サーバー)\u0026#39;!$A:$A, regexextract(\u0026#39;ログ用Create文(サーバー)\u0026#39;!$A:$A, \u0026#34;CREATE OR REPLACE TABLE\\s+`[^`]+\\.([^`]+)`\u0026#34;) = $A1), \u0026#34;\u0026#34;) [管理表]シート データ名 クエリ login ★ registration [ログ用Create文(サーバー)]シート CREATE OR REPLACE TABLE pj.datamart.login (… CREATE OR REPLACE TABLE loud-dev-45a0b.datamart.registration (… … [GSS] 複数の文字置換 lamda() // 多重ネストを回避したい =substitute(substitute(A1, \u0026#34;m\u0026#34;, \u0026#34;\u0026#34;), \u0026#34; \u0026#34;, \u0026#34;\u0026#34;) ↓ =reduce(A1, {\u0026#34;m\u0026#34;, \u0026#34; \u0026#34;}, lambda(acc, old, substitute(acc, old, \u0026#34;\u0026#34;))) mo ji oji cf. SUBSTITUTEをネストせずに複数置換するには スプレッドシートで空白を詰める便利関数はある？ 使い方と事例を紹介 OneChatMedia [GSS] 複数のファイル内特定シート統合 「A」 「B」 「C」 ↓ ↓ ↓ └--「D」--┘ ※　「」= GSSのファイルの意（便宜的） 前提として「A~C」ファイル内部シートのフォーマットは、統一していること （SQLの UNION ALL 結合方式をとりたい為） { coming soon …}\nGoogle Apps Script (=GAS) [GAS] 転置行列 /***** cf. https://docs.google.com/spreadsheets/d/1fQl5PQPaGcX4z9VP6gGkRHcBOku5Bn9_XfBpIzMPwXE/edit?gid=1244521797#gid=1244521797 *****/ function rearrangeData() { const sheet = SpreadsheetApp.getActiveSpreadsheet().getActiveSheet(); // アクティブなシートを取得 const data = sheet.getDataRange().getValues(); // シート内の全データを2次元配列として取得 let results = []; for (let i = 0; i \u0026lt; data.length; i += 2) { // データを2行ずつ処理 const code201 = data[i][0]; // 奇数行にある[201(found)]を取得 const code300 = data[i + 1] ? data[i + 1][0] : \u0026#39;\u0026#39;; // 偶数行にある[300(extracted)]を取得 results.push([code201, code300]); // [201][300]のペアを配列として保存 } // 結果をシートに書き込む（C列から） const startRow = 1; // 書き込み開始行 const startCol = 3; // 書き込み開始列（C列） sheet.getRange(startRow, startCol, results.length, 2).setValues(results); // 書き込み範囲に結果をセット } befor run no results 1 [201] 2 [300] 3 [201] 4 [300] after run no results results 1 [201] [300] 2 [201] [300] [GAS] 指定FMTでのクエリ自動生成 /***** cf. https://script.google.com/u/0/home/projects/1nQSocTOFaD_rYKe4kZWl9HJGHnQnH1LVxjv8bzsAUzB20EgbD3LiWrbf/edit?pli=1 *****/ const PROJECT_ID = \u0026#39;dev-project-XXXX\u0026#39;; const DATASET_ID = \u0026#39;datamart\u0026#39;; const TABLE_SUFFIX = \u0026#39;_20240801\u0026#39;; // メニュー追加 function onOpen() { let sheet = SpreadsheetApp.getActiveSpreadsheet(); let entries = [{ name: \u0026#34;Create文生成\u0026#34;, functionName: \u0026#34;generateBigQueryCreateStatements\u0026#34; }]; sheet.addMenu(\u0026#34;クエリ生成\u0026#34;, entries); } // メイン処理 function generateBigQueryCreateStatements() { try { let spreadsheet = SpreadsheetApp.openById(\u0026#39;~youer_sheet_id~\u0026#39;) let listSheet = spreadsheet.getSheetByName(\u0026#39;一覧\u0026#39;); let data = getSheetData(listSheet); // 対象テーブルごとに処理 let sqlStatements = data .filter(row =\u0026gt; isTableReady(row)) .map(row =\u0026gt; generateTableCreateStatement(spreadsheet, row)); // SQL文を別シートに出力 outputSqlStatements(sqlStatements); } catch (error) { Logger.log(`エラーが発生しました: ${error.message}`); } } // 一覧シートのデータを取得 function getSheetData(sheet) { let lastRow = sheet.getLastRow(); return sheet.getRange(2, 1, lastRow - 1, sheet.getLastColumn()).getValues(); } // 対象テーブルが対応済か確認 function isTableReady(row) { let statusColumnIndex = 12; // M列（実装ステータス） let tableNameColumnIndex = 6; // G列（テーブル物理名） let status = row[statusColumnIndex]; let tableName = row[tableNameColumnIndex]; return status === \u0026#39;対応済\u0026#39; \u0026amp;\u0026amp; tableName; } // 各テーブルのCREATE文を生成 function generateTableCreateStatement(spreadsheet, row) { let tableNameColumnIndex = 6; let tableName = row[tableNameColumnIndex]; let schemaSheet = spreadsheet.getSheetByName(tableName); if (!schemaSheet) { throw new Error(`テーブル ${tableName} に対応するシートが見つかりません。`); } let schemaData = getSchemaData(schemaSheet); let columns = schemaData .filter(schemaRow =\u0026gt; schemaRow[0] !== \u0026#39;\u0026#39; \u0026amp;\u0026amp; schemaRow[0] !== \u0026#39;他、共通カラム\u0026#39;) .map(schemaRow =\u0026gt; formatColumn(schemaRow)); return `CREATE OR REPLACE TABLE \\`${PROJECT_ID}.${DATASET_ID}.${tableName}${TABLE_SUFFIX}\\` (\\n ${columns.join(\u0026#39;,\\n \u0026#39;)}\\n);`; } // スキーマシートのデータを取得 function getSchemaData(sheet) { let schemaLastRow = sheet.getLastRow(); return sheet.getRange(2, 2, schemaLastRow - 1, 3).getValues(); // B〜D列のデータ取得 } // フォーマッター function formatColumn(schemaRow) { let columnName = schemaRow[0]; let dataType = schemaRow[1]; if (dataType.toLowerCase() === \u0026#39;float\u0026#39;) { dataType = \u0026#39;FLOAT64\u0026#39;; } let isNullable = schemaRow[2] === \u0026#39;可\u0026#39; || dataType.indexOf(\u0026#39;Array\u0026#39;) \u0026gt;= 0 ? \u0026#39;\u0026#39; : \u0026#39;NOT NULL\u0026#39;; return `\\`${columnName}\\` ${dataType} ${isNullable}`; } // SQL文出力 function outputSqlStatements(sqlStatements) { let ss = SpreadsheetApp.getActiveSpreadsheet(); let outputSheet = ss.getSheetByName(\u0026#39;ログ用Create文\u0026#39;) || ss.insertSheet(\u0026#39;ログ用Create文\u0026#39;); outputSheet.clear(); // 既存の内容をクリア outputSheet.getRange(1, 1, sqlStatements.length, 1).setValues(sqlStatements.map(stmt =\u0026gt; [stmt])); } [GAS] Googleカレンダーの特定イベント時間集計 /***** Context: …","date":1729569382,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1729569382,"objectID":"4dd7f15b07b94d1c63045583bc91e42a","permalink":"http://localhost:1313/todayilearned/post/googlesheets_used_functions/","publishdate":"2024-10-22T12:56:22+09:00","relpermalink":"/post/googlesheets_used_functions/","section":"post","summary":"📗 Make a memorandum of what you used with the most powerful free tool that anyone can use.","tags":["Google Sheets","Connected Sheets","GAS"],"title":"Google Sheets used in functions","type":"post"},{"authors":null,"categories":[""],"content":" Why 実務をしていると、、、\n「ある時点（eg.10/14）のデータが欠損しているように思われる。 原因究明してほしいのと、 できたら分析で使うから戻して（データ復旧して）ほしい！」\nこんな依頼がくる。\nデータ分析とはまた違う、データの品質を救う技術のアレコレ。\nWhat 手法 用途 backfill 埋める timetravel 戻れる Backfill Backfillとは、データの過去分を遡って埋める作業を指す。\neg. 新しいデータパイプラインを導入した後に、そのパイプラインを使って過去のデータを処理・格納するために行われる。 データ欠損が発生した場合や、データが遅延している場合にもbackfillが使われる。 【4選】BigQueryでbackfillを行う:\nクエリベースで再計算: 過去のデータを対象とするクエリを実行し、その結果をテーブルに追加挿入（INSERT INTO）。 スケジューラの活用: DataflowやAirflowなどのワークフローオーケストレーションツールを使い、特定の過去の期間のデータを処理してBigQueryに取り込む。 Partitioned Tables: BigQueryのパーティションテーブルを使用して、特定の時間範囲のデータだけを効率よく処理。パーティションキー（通常は日時）を指定し、そのパーティション内のデータをbackfill。 Cloud FunctionsやPub/Sub: 外部のクラウドサービスやトリガーを使って、BigQueryにデータをbackfillも可能。 Timetravel timetravelは、BigQueryの「過去のデータにアクセスする機能」。 BigQueryでは、データを挿入・更新・削除した後も、　最大7日間の履歴にアクセスできる機能が提供されている。 これにより、過去の状態のテーブルをクエリして復元•分析ができる。\n-- 1時間前の状態のデータにアクセス select * from `project.dataset.table` FOR SYSTEM TIME AS OF timestamp_sub(current_timestamp(), interval 1 hour) 期間制限: 過去7日間のみの履歴が保持されているため、それより前のデータにアクセスすることはできない。 クエリコスト: 通常のクエリと同様に、タイムトラベルクエリもデータのスキャンにコストがかかるため、注意が必要。 CDC (Change Data Capture) CDCは、データベースの変更をリアルタイムでキャプチャし、他のシステムやデータストアに反映させる技術。 backfillとは異なり、データの変更を逐次反映するため、データの欠損や遅延が少なく、リアルタイム性が必要な場面に適している。\nbackfillの補完的: backfillでは過去のデータを埋めることが主目的、CDCは継続的にデータを同期が目的。両者を組み合わせることで、リアルタイムのデータストリームに欠損があった場合に後からbackfillで補完する、といった運用が可能。 運用の複雑さ: CDCを正確に実装するには、データの一貫性やレイテンシーを管理するための仕組みが必要。 Snapshot スナップショットは、データベースやテーブルのある時点での状態を保存しておく方法。\n定期的にスナップショットを取得することで、timetravelのように過去のデータを参照できるが注意点がある。\ntimetravelの代替: BigQueryのtimetravelは最大7日間の履歴しか保持できないが、スナップショットはもっと長期間保存可能。定期的なスナップショットを取得しておけば、timetravel期間外の過去データにもアクセス可能。 ストレージコスト: スナップショットを頻繁に取得するとストレージのコストが増大！！！… 適切な保存期間と頻度の設計が重要。 Partitioning パーティショニングは、テーブルを特定のカラム（日時など）に基づいて分割し、データ管理を効率化する手法。\nData Validation データの官署🚪 データバリデーションは、データの一貫性や正確性を検証するプロセス。 データパイプラインの各段階でバリデーションを行うことで、異常なデータが流入することを防ぐ。\nData Lineage データの探偵🕵️ データリンネージは、データの出所や変更履歴を追跡する仕組み。 どのようなプロセスを経てデータが現在の状態になったのかを把握できるため、データの透明性や信頼性を確保。\ntimetravelの補完: timetravelが特定の時点のデータを参照するのに対して、データリンネージはデータがどのように加工され、変わってきたかを追跡するため、過去の変更の影響を評価できる。 エラートラッキング: backfillやデータパイプラインで異常が発生した際、データリンネージを使用して、どの処理が原因だったのかを特定することが可能。 Audit Logging (監査ログ) データの警察👮 監査ログは、データアクセスやクエリ実行の履歴を記録する機能。 BigQueryや他のクラウドサービスでは、どのユーザーがいつどのデータにアクセスしたかを追跡できるため、セキュリティやコンプライアンスの観点で重要。\nETL/ELT Pipeline データの抽出、変換、ロードのプロセス。\n用途：backfill時や、定期的なデータ更新 過去データの抽出と変換を実施し、適切な形でDWHにロードする一連の処理を自動化。 大規模なデータ処理が必要な場合、効率的にbackfillやデータ管理できるETLツールやデータパイプライン管理ツール ツール： Airflow, Dataflow, dbt …etc. Ref. ","date":1729054194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1729054194,"objectID":"e7f0031aaca6b617445c0ff4b8a31d80","permalink":"http://localhost:1313/todayilearned/post/bigquery_about_data_quality/","publishdate":"2024-10-16T13:49:54+09:00","relpermalink":"/post/bigquery_about_data_quality/","section":"post","summary":"🔍 Different perspective from data analysis. About data quality.","tags":["BigQuery","Data Reliability Engineering","DWH"],"title":"BigQuery about Data Quality","type":"post"},{"authors":null,"categories":[""],"content":" Why remote -[main] repository ----------------------------------------------------\u0026gt; | ❶/clone　↑ ⑤/push | ↓ ↑ ③/commit -- ↑ ④/commit -┘ ↓ ❻/pull local [main] branch ----------|-------------|----------------------\u0026gt; | ❷/chackout　| /add | /add └-\u0026gt; Aさん[feature_A] branch ------┘ What Official: https://git-scm.com | doc | com | dw GitHub: https://github.com/GIT ワークフローという哲学 ワークフロー名 ブランチ名（種） 対応規模 GitHub flow 2種(master/main, feature） 個人 ~ 小規模 Git-flow 5種(master/main, decelop, feature, release, hot-fix) 大規模 企業規模や、OSS単位ごとで使用される粒度が異なる。（独自なところもあるが上記2つが主流の流派）\n# 現在`master`ブランチに居ることを確認して $ git branch * master $ git cheackout -b feature/edit_hoge ie.\nfeature/{任意}} ブランチ名を切り出す。 わざわざ明記しなくとも動く。ただ、このように明示的にしていることで開発にかかる他の方に迷惑を被ることを回避できる。 cf.\nGitHub flow GitHub 【入門】Github Flowとは？使い方の基本 カゴヤのサーバー研究室 What is Gitflow? Atlassian Git-flowをざっと整理してみた DevoloperIO 幹から枝を # main/master branch から新たにブランチを切り出したい。 local -[main] branch ------------------------\u0026gt; └├--\u0026gt; Aさん[feature_A] branch ---\u0026gt; └--\u0026gt; Bさん[feature_B] branch ---\u0026gt; $ git checkout -b feature_B 枝を手元に # local -[main] branch ------------------------------------------\u0026gt; └-\u0026gt; Aさん[feature_A] branch -----------------------\u0026gt; └-\u0026gt; Bさん[feature_A] branch ----\u0026gt; # 他の人のブランチの最新の変更を取得して、リモート上のブランチ名を一覧確認 $ git fetch origin $ git branch -r # feature_Aブランチを自分のローカルにチェックアウト $ git checkout -b feature_A origin/feature_A $ git branch # 変更を加え、コミット＆プッシュ $ git add . $ git commit -m \u0026#34;プルリクエストのための変更\u0026#34; $ git push origin feature 枝から枝を # main/master branch に merge されていない 他の方の branch に対してプルリクエストを作成したい。 local -[main] branch ------------------------------------------\u0026gt; └-\u0026gt; Aさん[faeture_A] branch -----------------------\u0026gt; └-\u0026gt; Bさん[feature_B] branch -┘ # 他の人のブランチの最新の変更を取得するためにリモートリポジトリからフェッチ $ git fetch origin \u0026lt;その他の人のブランチ名\u0026gt; # 他の人のブランチをローカルに持ってくるために、チェックアウト $ git checkout -b \u0026lt;新しいブランチ名\u0026gt; origin/\u0026lt;その他の人のブランチ名\u0026gt; # 変更を加え、コミット＆プッシュee $ git add . $ git commit -m \u0026#34;プルリクエストのための変更\u0026#34; $ git push origin \u0026lt;新しいブランチ名\u0026gt; 枝から枝を（名前変更） # branch名を[feature_B] → [feature_C] に変えたい local -[main] branch -------------------------------------------------\u0026gt; └-\u0026gt; Aさん[faeture_A] branch ------------------------------\u0026gt; └-\u0026gt; Bさん[feature_B] branch -\u0026gt; [feature_C] branch ----\u0026gt; # 現在ブランチ確認 $ git branch * feature_B main # 現在ブランチ名をリネーム $ git branch -m feature_C # 新しいブランチ名をリモートにプッシュ反映 $ git push origin feature_C # 古いブランチ名はリモートから削除（後片付け） $ git push origin --delete feature_B # 他の開発者への通知 $ git fetch origin 大きく育った枝を剪定 # [feature_A] の修正ファイル数が膨大になり分割して、切り出しor並び替えしたい -[main] branch -------------------------------------------\u0026gt; └-\u0026gt; Aさん[faeture_A] branch　| | | |/rebase | | | └-\u0026gt; [new-branch-1] branch -┘ | | └-\u0026gt; [new-branch-2] branch ---|-┘ └-\u0026gt; [new-branch-3] branch ---┘ └-\u0026gt; [new-branch-4] branch ------× コミット分割（※ 必要に応じて） # コミットをいくつかに分割したい場合、インタラクティブリベースでコミットを分割 # （ n は分割したいコミットの数） $ git rebase -i HEAD~n 分割したいコミットの行を edit に変更。(エディタが開き、コミット履歴が表示されます。) pick e7f6c98 First commit edit d4f8a7b Second commit pick 7a7f6e3 Third commit # editにしたコミットが表示されたら、そのコミットを分割 # コミットを一旦解除 $ git reset HEAD^ $ git add \u0026lt;ファイル名\u0026gt; $ git commit -m \u0026#34;細かく分けたコミット1\u0026#34; $ git add \u0026lt;別のファイル名\u0026gt; $ git commit -m \u0026#34;細かく分けたコミット2\u0026#34; すべてのコミットを分割した後、リベースを続行 $ git rebase --continue ブランチ分割 異なるPRを作成するために、複数のブランチに変更を振り分け。 新しいブランチを作成 # 現在ブランチから分割する新しいブランチを作成。 $ git checkout -b new-branch-1 特定のコミットを含むようにブランチを操作 新しいブランチに特定のコミットだけを残すために、git resetやgit cherry-pickを使う。 eg. # 新しいブランチに特定のコミットだけを残したい場合 $ git reset --hard \u0026lt;特定のコミットハッシュ\u0026gt; # 不要なコミットを削除したい場合（新しいブランチに残したくないコミットがあれば） $ git rebase -i 各ブランチでPR作成 それぞれのブランチで、GitHubにプッシュして新しいPRを作成。 $ git push origin new-branch-1 GitHub上で、この新しいブランチに基づいたPRを作成します。 次に、他のブランチも同様にプッシュし、それぞれにPRを作成します。 古くなった枝を最新に remote -[main] branch -----------------------------------------\u0026gt; |/pull |/pull local -[main] branch ---------------------|------------|------\u0026gt; | | | | ↓/merge ↓/rebase └\u0026gt; Aさん[faeture_A] branch ---✨-----------✨------\u0026gt; [Merge] 簡単かつ安全（でも履歴汚ねぇ…）\n最新のmain（またはdevelop）ブランチを取得 $ git checkout main $ git pull origin main 作業ブランチに移動し、マージ $ git checkout feature_A $ git merge main (if) マージコンフリクトの解消 $ git add \u0026lt;conflicted-files\u0026gt; $ git commit -S -m \u0026#34;memo\u0026#34; 変更をリモートにプッシュ $ git push origin feature_A [Rebase] 綺麗な履歴\nmainブランチの最新のコミットの後に来るように並べ替えることができる。 これにより、コミット履歴が綺麗な状態で最新コードを取り込むことができる。 一方で、特に共有ブランチに対して行う場合は強制プッシュが必要になる。（他の開発者に影響が出る可能性あり）\n$ git checkout main $ git pull origin main $ git checkout feature_A $ git rebase main (if) リベース中にコンフリクトが発生した場合 $ git add \u0026lt;conflicted-files\u0026gt; $ git rebase --continue # 中断したい場合 $ git rebase --abort リモートに強制プッシュ $ git push --force-with-lease origin feature_A 空コミットの要請 $ git commit --allow-empty -m \u0026#34;メッセージ\u0026#34; cf. git commit –allow-empty で空コミットを作成してPR作るといい感じ HatenaBlog 【Github】空コミットをpushする hosochinの技術ブログ How to Push an Empty Commit in Git FreeCodeCamp GPG (署名付きコミット) ↑/commit :GPG 「わいやで」 -[main] branch　------------------------|----------------\u0026gt; └\u0026gt; Aさん[faeture_A] branch -------┘ GitのGPGは、Gitにおいてコミットやタグなどの操作に署名を付けるための仕組み。 GPG（GNU Privacy Guard）は、デジタル署名や暗号化を行うためのツール。 GitのGPGを使用することで、コミットやタグが本当に信頼できるものであることを確認することができる。 # Homebrewで下準備 $ brew install gnupg pinentry-mac # GPGキーペアの生成 $ gpg --full-generate-key # 公開鍵の取得 $ gpg …","date":1727168888,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1727168888,"objectID":"2b59ee12ceec1c29f251377ab4d31dbb","permalink":"http://localhost:1313/todayilearned/post/github_branches_pruning/","publishdate":"2024-09-24T18:08:08+09:00","relpermalink":"/post/github_branches_pruning/","section":"post","summary":"📍When developing on GitHub, I often switch branches, just like pruning.","tags":["Git","GitHub"],"title":"Git Branches pruning","type":"post"},{"authors":null,"categories":[""],"content":" Why 自作プラグインを作っってみたい。\nWhat Howto Build Vimのプラグインは自作可能です。 Vimスクリプトを使えば、Vimの機能を拡張したり、自分好みの機能を追加することができます。 作成手順としては、以下の流れで進められます。\nVimスクリプトの知識 基本的なVimのコマンドや設定（:set ,:map …etc.）の理解 Vimスクリプトの構文（if, function, autocmd…etc.）の理解 ie. Vimプラグイン開発入門 Zenn プラグインのディレクトリ構成 一般的なVimプラグインは以下のような構成で作成される。 (eg. GitHub上で管理する場合、プラグイン名でディレクトリを作り、以下のようなサブディレクトリを配置) myplugin/ ├── autoload/ \u0026lt;-関数が初めて呼び出されたときにロードされるスクリプト ├── doc/ \u0026lt;-プラグインのメイン機能を定義するスクリプト ├── plugin/ \u0026lt;-プラグインのメイン機能を定義するスクリプト ├── syntax/ \u0026lt;-プラグインのメイン機能を定義するスクリプト ├── README.md └── LICENSE プラグインの機能定義 基本的な設定やコマンドを定義。 function! MyPluginFunction() echo \u0026#34;Hello from my plugin!\u0026#34; endfunction command! MyPlugin call MyPluginFunction() :MyPlugin と入力することで関数が実行され、「Hello from my plugin!」と表示される。 プラグインの公開 作成したプラグインはGitHubなどに公開して、他のユーザーがインストールして使えるようにすることができます。 README.mdにインストール手順や使い方を記載しておくと良いです。 Vimプラグインの作成は、最初はシンプルな機能から始めて、徐々に機能を拡張していくのが良いでしょう。 自作している方々 vim-textmanip Winresizer nin-english.vim 便利なプラグイン一覧 dein.vim unite.vim Molokai Indent Guides NERDTree fugitive.vim Search Multiple Search vim-quickhl vim-visualstar ctrlp.vim 開発補助 emmet-vim neocomplete.vim Neosnippet closetag.vim tcomment surround.vim vim-textmanip DrawIt vim-prettyprint html5validator syntastic watchdogs.vim vim-operator-sort vim-operator-comment Ref. 機能別おすすめVimプラグイン25選！入れ方も解説 ","date":1726051189,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726051189,"objectID":"356e3db96dfecea55ac51c3c87092ef7","permalink":"http://localhost:1313/todayilearned/post/vim_building_plugin/","publishdate":"2024-09-11T19:39:49+09:00","relpermalink":"/post/vim_building_plugin/","section":"post","summary":"✅ Once I gain a deeper understanding, what may be able to develop general-purpose automated tools.","tags":["Vim","Howto","Vim script"],"title":"Vim Building plugin","type":"post"},{"authors":null,"categories":[""],"content":" Why SSIA\nWhat Argo Projects Argo(アルゴ）は、2017年にApplatixで開発された。 その後、Applatixは2018年にIntuitによって買収された。 2020年4月にCNCFのインキューベーション・プロジェクト（支援）になった。\nアルゴプロジェクトは、ワークフロー、デプロイ、ロールアウト、およびイベントなど、 ジョブやアプリケーションをデプロイし、実行するためのKubernetes-ネイティブツールのセットを提供。 継続的デリバリー、累進的デリバリーなどの GitOps パラダイムによって、KubernetesでMLOps つまり機械学習のDevOps基盤を推進。\nTekton Pipeline や Trigger のタスクの１ステップに、アルゴを組み込むことができるので、 Tektonのトリガーで受けた git push の イベントから、Argo CDにつないでアプリケーションの状態管理など連携運用も可能。\nこの活動は複数あるが、主要な4つのグループをまとめると以下\nArgo Workflows Kubernetesクラスター上でワークフローを定義・実行できるオープンソースのシステム。（ワークフローは、複数のタスクが順序や依存関係に基づいて実行されるプロセスのこと） 各タスクはPod（Kubernetes上のコンテナ）として実行されるため、分散処理が簡単に実現でき、大規模なデータ処理や複雑な計算フローにも対応可能。 メリット コンテナベース 各ステップが独立したコンテナとして実行されるため、タスクの依存関係を管理しやすい リソースの分離も可能 スケーラビリティ 負荷に応じてワークフローのタスクを自動的にスケールさせることができる（Kubernetesのスケーリング機能活用） 再現性 ワークフロー定義はYAMLファイルで記述され、バージョン管理が可能 同じワークフローを再実行したり、環境間での移植が容易 可視化とモニタリング ワークフローの状態や進行状況をWeb UIで可視化 失敗したタスクの再実行も Web UIで簡単に行える 依存関係の管理 タスク同士の依存関係をDAG（Directed Acyclic Graph）で表現でき、順次処理や並列処理を柔軟に設定可能 ワークフロー構造（サンプル）ref apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: hello-world- spec: entrypoint: main templates: - name: main steps: - - name: print-hello template: echo - name: echo container: image: alpine:latest command: [echo, \u0026#34;Hello, Argo!\u0026#34;] cf. https://argo-workflows.readthedocs.io/en/latest/ Artifacts Management in Container-Native Workflows (Part 1) Quick Start Argo CD(Continuous Development) cf. Getting Started Argo Rollouts cf. Getting Started demo Argo Events cf. Introduction cf. Argo とはなんだ？ Qiita\n^ IBMの方がより簡潔にまとめられている! Local Hans-on Goal 「embulkでローカルにあるcsvファイルを読み取って、内容をフィルターして標準出力に出すところまで」 Prep kubernetesクラスタのセットアップ(ローカル) $ brew install kubectl $ kubectl version --client Client Version: v1.29.2 Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 cf. kubectlの基本操作をまとめてみた。 Qiita Argo Workflowsのインストール ref # Namespaceの作成 $ kubectl create ns argo # 作成したNamespaceにArgoをインストールする $ kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo/stable/manifests/quick-start-postgres.yaml NAME READY STATUS RESTARTS AGE argo-server-74d5fbc96f-9gc2n 1/1 Running 0 2m2s minio-58977b4b48-m9c5l 1/1 Running 0 2m1s postgres-6b5c55f477-pbbv8 1/1 Running 0 2m1s workflow-controller-7fb47d49bb-t2mrb 1/1 Running 0 2m Argo CLIのインストール $ argo version argo: v3.3.6 BuildDate: 2022-05-26T01:07:24Z GitCommit: 2b428be8001a9d5d232dbd52d7e902812107eb28 GitTreeState: clean GitTag: v3.3.6 GoVersion: go1.17.10 Compiler: gc Platform: darwin/amd64 Implementation（実装） Overview . ├── Dockerfile ├── argo/ │ ├── workflow.yaml │ └── workflow_template.yaml └── data/ └── customer.csv.gz DockerfileにEmbulkの実行環境、フィルタリングに必要なpluginも同時にインストールするよう記述。 FROM openjdk:8-jre-alpine ARG VERSION RUN mkdir -p /root/.embulk/bin \\ \u0026amp;\u0026amp; wget -q https://dl.embulk.org/embulk-${VERSION}.jar -O /root/.embulk/bin/embulk \\ \u0026amp;\u0026amp; chmod +x /root/.embulk/bin/embulk ENV PATH=$PATH:/root/.embulk/bin RUN apk add --no-cache libc6-compat \u0026amp;\u0026amp; embulk gem install embulk-filter-column Embulkを実行する Templateは下記のように記述。 (※ Argo Workflows はこのように処理をテンプレ化させておいて、各所で再利用することが可能らしい。) apiVersion: argoproj.io/v1alpha1 kind: WorkflowTemplate metadata: name: embulk-template spec: entrypoint: embulk templates: - name: embulk inputs: artifacts: - name: embulk-config path: /input/config.yml container: image: embulk:v0.1 command: [java] args: [\u0026#34;-jar\u0026#34;, \u0026#34;/root/.embulk/bin/embulk\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;/input/config.yml\u0026#34;] volumeMounts: - name: data mountPath: /data volumes: - name: data configmap: name: customer-data embulkを実行する本体の処理は以下。embulkのconfigはとりあえずartifactsとして渡す。 apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: name: csv-to-stdout-sample spec: entrypoint: embulk arguments: artifacts: - name: embulk-config raw: data: | in: type: file path_prefix: \u0026#39;data/\u0026#39; decoders: - {type: gzip} parser: charset: UTF-8 newline: CRLF type: csv delimiter: \u0026#39;,\u0026#39; quote: \u0026#39;\u0026#34;\u0026#39; escape: \u0026#39;\u0026#34;\u0026#39; null_string: \u0026#39;NULL\u0026#39; skip_header_lines: 1 columns: - {name: customer_id, type: string} - {name: customer_name, type: string} - {name: gender_cd, type: string} - {name: gender, type: string} - {name: birth_day, type: timestamp, format: \u0026#39;%Y-%m-%d\u0026#39;} - {name: age, type: long} - {name: postal_cd, type: string} - {name: address, type: string} - {name: application_store_cd, type: string} - {name: application_date, type: string} - {name: status_cd, type: string} filters: - type: column columns: - {name: customer_id, type: string} - {name: gender_cd, type: string} - {name: gender, type: string} - {name: birth_day, type: timestamp, format: \u0026#39;%Y-%m-%d\u0026#39;} - {name: age, type: long} - {name: postal_cd, type: string} - {name: application_store_cd, type: string} - {name: application_date, type: string} - {name: status_cd, type: string} out: type: stdout workflowTemplateRef: name: embulk-template Execution DockerfileはイメージとしてArgoから呼び出せるように登録 （2022年6月：0.9.24stableとのこと） $ docker build . -t embulk:v0.1 --build-arg VERSION=0.9.24 dataはConfigmapで渡す。 $ kubectl -n argo create configmap customer-data --from-file=data/customer.csv.gz ワークフローを登録して処理が正しく動くか確認 # テンプレートを登録 $ argo -n argo …","date":1725853259,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725853259,"objectID":"411a4d24f5ca2c42e9d63c2cd8112075","permalink":"http://localhost:1313/todayilearned/post/argo_workflows_catchup/","publishdate":"2024-09-09T12:40:59+09:00","relpermalink":"/post/argo_workflows_catchup/","section":"post","summary":"🐙 Understanding and implementing Argo projects and workflows.","tags":["Argo Workflows","DAG","Kubernetes"],"title":"Argo Workflows Catchup","type":"post"},{"authors":null,"categories":[""],"content":" Why 分析関係でお仕事をしていたり、最近では営業担当や、マーケティング担当の方々も容易にBigQueryで用いるのがDQL。 他にも様々な区分のなコマンドが存在している。\nコマンド区分の理解と、用いてきた関数等を覚書していきたい。\nWhat SQL Commands Classification DDL: Data Define Language CREAT ALTER DROP RENAME TRUNCATE DML: Data Manipulation Language INSERT UPDATE DEFELR MERGE DCL: Data Control Language GRANT REVOKE DQL: Data Query Language SELECT SQL Types 「文」「句」「式」の違い\nUsed Functions [小技] テーブルフラット化 #standardSQL /* cf. * GA4/Firebaseのログをフラット化する汎用クエリ * https://www.marketechlabo.com/ga4-firebase-log-preprocessing/ */ with #importTable GA4_EVENTS as (select * from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`) #preprocessTable --ログフラット化 , GA4_LOG_FLAT as ( select user_pseudo_id , event_name as EVENT_NAME , (select -- 如何なる型を、文字列に纏め上げ case when p.value.string_value is not NULL then safe_cast(p.value.string_value as string) when p.value.int_value is not NULL then safe_cast(p.value.int_value as string) when p.value.double_value is not NULL then safe_cast(p.value.double_value as string) else NULL end from unnest(event_params) p where p.key = \u0026#39;page_title\u0026#39;) as PAGE_TITLE from GA4_EVENTS ) #outputTable select * from GA4_LOG_FLAT; フラット化を動的処理 #standardSQL /* cf. * GA4/Firebaseのログをフラット化する汎用クエリ * https://www.marketechlabo.com/ga4-firebase-log-preprocessing/ */ #config declare str_ep_columns string; declare str_up_columns string; declare str_dynamic_columns string; --event_params set str_ep_columns = ( with #importTable GA4_EVENTS as (select * from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`) #preprocess , TYPE_CHACK as ( select KEY , case when 0 \u0026lt; CNT_STRING then \u0026#39;string\u0026#39; when 0 \u0026lt; CNT_INT64 and 0 \u0026lt; CNT_FLOAT64 then \u0026#39;numeric\u0026#39; when 0 \u0026lt; CNT_INT64 then \u0026#39;int64\u0026#39; when 0 \u0026lt; CNT_FLOAT64 then \u0026#39;float64\u0026#39; else \u0026#39;string\u0026#39; end TYPE --型 from ( select p.key as KEY , sum(case when p.value.string_value is not null then 1 else 0 end) as CNT_STRING , sum(case when p.value.int_value is not null then 1 else 0 end) as CNT_INT64 , sum(case when p.value.double_value is not null then 1 else 0 end) as CNT_FLOAT64 from GA4_EVENTS, unnest(event_params) as p group by 1 ) ) , GET_LOG_FLAT as ( select /* --下記のクエリを「KEY」の数だけ生成 * (select * case * when p.value.string_value is not null then safe_cast(p.value.string_value as string) * when p.value.int_value is not null then safe_cast(p.value.int_value as string) * when p.value.double_value is not null then safe_cast(p.value.double_value as string) * else null * end * from unnest(event_params) p where p.key = \u0026#34;all_data\u0026#34;) as all_data */ string_agg( \u0026#39;(select case when p.value.string_value is not null then safe_cast(p.value.string_value as \u0026#39; || TYPE || \u0026#39;) when p.value.int_value is not null then safe_cast(p.value.int_value as \u0026#39; || TYPE || \u0026#39;) when p.value.double_value is not null then safe_cast(p.value.double_value as \u0026#39; || TYPE || \u0026#39;) else null end from unnest(event_params) p where p.key = \u0026#34;\u0026#39; || KEY || \u0026#39;\u0026#34;) as \u0026#39; || KEY order by KEY --※順序規定 ) from TYPE_CHACK ) select * from GET_LOG_FLAT ); --user_properties set str_up_columns = ( with #impoerTable GA4_EVENTS as (select * from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`) #preprocess , TYPE_CHACK as ( select KEY , case when 0 \u0026lt; CNT_STRING then \u0026#39;string\u0026#39; when 0 \u0026lt; CNT_INT64 and 0 \u0026lt; CNT_FLOAT64 then \u0026#39;numeric\u0026#39; when 0 \u0026lt; CNT_INT64 then \u0026#39;int64\u0026#39; when 0 \u0026lt; CNT_FLOAT64 then \u0026#39;float64\u0026#39; else \u0026#39;string\u0026#39; end TYPE --型 from ( select p.key as KEY , sum(case when p.value.string_value is not null then 1 else 0 end) as CNT_STRING , sum(case when p.value.int_value is not null then 1 else 0 end) as CNT_INT64 , sum(case when p.value.double_value is not null then 1 else 0 end) as CNT_FLOAT64 from GA4_EVENTS, unnest(user_properties) p group by 1 ) ) , GET_LOG_FLAT as ( select /* --下記のクエリを「KEY」の数だけ生成 * (select * case * when p.value.string_value is not null then safe_cast(p.value.string_value as string) * when p.value.int_value is not null then safe_cast(p.value.int_value as string) * when p.value.double_value is not null then safe_cast(p.value.double_value as string) * when p.value.set_timestamp_micros is not null then safe_cast(p.value.set_timestamp_micros as string) * else null * end * from unnest(user_properties) p where p.key = \u0026#34;all_data\u0026#34;) as all_data */ string_agg( \u0026#39;(select case when p.value.string_value is not null then safe_cast(p.value.string_value as \u0026#39; || type || \u0026#39;) when p.value.int_value is not null then safe_cast(p.value.int_value as \u0026#39; || type || \u0026#39;) when p.value.double_value is not null then safe_cast(p.value.double_value as \u0026#39; || type || \u0026#39;) when p.value.set_timestamp_micros is not null then safe_cast(p.value.set_timestamp_micros as \u0026#39; || type || \u0026#39;) else null end from unnest(user_properties) p where p.key = \u0026#34;\u0026#39; || key || \u0026#39;\u0026#34;) u_\u0026#39; || key order by KEY --順序規定 ) from TYPE_CHACK ) select * from GET_LOG_FLAT ); #aggregation if 0 \u0026lt; length(str_up_columns) then set str_dynamic_columns = str_ep_columns || \u0026#39;, \u0026#39; || str_up_columns; else set str_dynamic_columns = str_ep_columns; end if; #output execute immediate format(\u0026#34;\u0026#34;\u0026#34; create or replace table …","date":1724928464,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1724928464,"objectID":"e44ae9e80925024cbf85f170637acfdd","permalink":"http://localhost:1313/todayilearned/post/bigquery_used_functions/","publishdate":"2024-08-29T19:47:44+09:00","relpermalink":"/post/bigquery_used_functions/","section":"post","summary":"🔍 When I getting from my daily work. Added as need. I keep writing various tips.","tags":["BigQuery","SQL","Data Analytics"],"title":"BigQuery Used in functions","type":"post"},{"authors":null,"categories":[""],"content":" Why SSIA\nWhat Trocco とは Official: https://trocco.io/ LP: https://trocco.io/lp/index.html GitHub: https://github.com/trocco-io Trocco 転送ログ eg.\n[Spanner] ------\u0026gt; 「 Troccco -\u0026gt; [BigQuery] [DataSource] ---\u0026gt; 」 2024-08-20 15:06:00.298 +0000 Preparing your trocco environment... 2024-08-20 15:06:00.347 +0000 cpu_request=3.5, memory_request=15.0Gi, cpu_limit=3.5, memory_limit=15.0Gi, disk=200.0Gi Successfully created your environment Loading Java Agent version 1 (using ASM9). 2024-08-20 15:07:27.489 +0000: Embulk v0.9.26 2024-08-20 15:07:27.971 +0000 [WARN] (main): DEPRECATION: JRuby org.jruby.embed.ScriptingContainer is directly injected. 2024-08-20 15:07:29.165 +0000 [INFO] (main): BUNDLE_GEMFILE is being set: \u0026#34;/work/embulk_bundle/Gemfile\u0026#34; 2024-08-20 15:07:29.166 +0000 [INFO] (main): Gem\u0026#39;s home and path are being cleared. 2024-08-20 15:07:30.636 +0000 [INFO] (main): Started Embulk v0.9.26 2024-08-20 15:07:30.808 +0000 [INFO] (0001:transaction): Loaded plugin embulk-input-gcs (0.3.4) 2024-08-20 15:07:32.458 +0000 [INFO] (0001:transaction): Loaded plugin embulk-output-bigquery (0.6.9.trocco.0.1.0) 2024-08-20 15:07:32.470 +0000 [INFO] (0001:transaction): Loaded plugin embulk-filter-speedometer (0.3.6) 2024-08-20 15:07:32.482 +0000 [INFO] (0001:transaction): Loaded plugin embulk-filter-column (0.7.1) 2024-08-20 15:07:32.492 +0000 [INFO] (0001:transaction): Loaded plugin embulk-filter-typecast (0.2.2) 2024-08-20 15:07:33.305 +0000 [INFO] (0001:transaction): Using local thread executor with max_threads=8 / output tasks 4 = input tasks 1 * 4 2024-08-20 15:07:33.327 +0000 [INFO] (0001:transaction): embulk-output-bigquery: Get dataset... sample-dev-45a0b:amount_master_data 2024-08-20 15:07:33.329 +0000 [WARN] (0001:transaction): embulk-output-bigquery: timeout_sec is deprecated in google-api-ruby-client \u0026gt;= v0.11.0. Use read_timeout_sec instead 2024-08-20 15:07:34.085 +0000 [INFO] (0001:transaction): embulk-output-bigquery: Create table... sample-dev-45a0b:amount_master_data.LOAD_TEMP_32196187_5779_43e4_a4e5_9caa550aa7cb_act_present_item 2024-08-20 15:07:34.257 +0000 [INFO] (0001:transaction): embulk-output-bigquery: Create table... sample-dev-45a0b:amount_master_data.act_present_item 2024-08-20 15:07:34.380 +0000 [INFO] (0001:transaction): {done: 0 / 1, running: 0} 2024-08-20 15:07:34.423 +0000 [INFO] (0020:task-0000): {speedometer: {active: 0, total: 0.0b, sec: 0.00, speed: 0.0b/s, records: 0, record-speed: 0/s}} 2024-08-20 15:07:34.633 +0000 [INFO] (0020:task-0000): embulk-output-bigquery: create /tmp/bigquery_output_option_20240820-1-exorlf.16.2000.jsonl.gz 2024-08-20 15:07:34.642 +0000 [INFO] (0020:task-0000): {speedometer: {active: 0, total: 30.0b, sec: 0.22, speed: 137b/s, records: 1, record-speed: 4/s}} 2024-08-20 15:07:34.643 +0000 [INFO] (0001:transaction): {done: 1 / 1, running: 0} 2024-08-20 15:07:34.650 +0000 [INFO] (Ruby-0-Thread-1: /work/embulk_bundle/jruby/2.3.0/gems/embulk-output-bigquery-0.6.9.trocco.0.1.0/lib/embulk/output/bigquery/bigquery_client.rb:153): embulk-output-bigquery: Load job starting... job_id:[embulk_load_job_a2b456a3-99f7-4152-8a66-8bc28e994f79] /tmp/bigquery_output_option_20240820-1-exorlf.16.2000.jsonl.gz =\u0026gt; sample-dev-45a0b:amount_master_data.LOAD_TEMP_32196187_5779_43e4_a4e5_9caa550aa7cb_act_present_item in asia-northeast1 2024-08-20 15:07:34.651 +0000 [WARN] (Ruby-0-Thread-1: /work/embulk_bundle/jruby/2.3.0/gems/embulk-output-bigquery-0.6.9.trocco.0.1.0/lib/embulk/output/bigquery/bigquery_client.rb:153): embulk-output-bigquery: timeout_sec is deprecated in google-api-ruby-client \u0026gt;= v0.11.0. Use read_timeout_sec instead 2024-08-20 15:07:35.845 +0000 [INFO] (Ruby-0-Thread-1: /work/embulk_bundle/jruby/2.3.0/gems/embulk-output-bigquery-0.6.9.trocco.0.1.0/lib/embulk/output/bigquery/bigquery_client.rb:153): embulk-output-bigquery: Load job checking... job_id:[embulk_load_job_a2b456a3-99f7-4152-8a66-8bc28e994f79] elapsed_time:7.4e-05sec status:[RUNNING] 2024-08-20 15:07:45.899 +0000 [INFO] (Ruby-0-Thread-1: /work/embulk_bundle/jruby/2.3.0/gems/embulk-output-bigquery-0.6.9.trocco.0.1.0/lib/embulk/output/bigquery/bigquery_client.rb:153): embulk-output-bigquery: Load job completed... job_id:[embulk_load_job_a2b456a3-99f7-4152-8a66-8bc28e994f79] elapsed_time:10.056669999999999sec status:[DONE] 2024-08-20 15:07:45.900 +0000 [INFO] (Ruby-0-Thread-1: /work/embulk_bundle/jruby/2.3.0/gems/embulk-output-bigquery-0.6.9.trocco.0.1.0/lib/embulk/output/bigquery/bigquery_client.rb:153): embulk-output-bigquery: Load job response... job_id:[embulk_load_job_a2b456a3-99f7-4152-8a66-8bc28e994f79] response.statistics:{:start_time=\u0026gt;1724166455760, :completion_ratio=\u0026gt;1, :creation_time=\u0026gt;1724166455640, :end_time=\u0026gt;1724166456927, :reservation_id=\u0026gt;\u0026#34;default-pipeline\u0026#34;, :total_slot_ms=\u0026gt;152, :load=\u0026gt;{:input_file_bytes=\u0026gt;116, :output_bytes=\u0026gt;73, :output_rows=\u0026gt;1, :bad_records=\u0026gt;0, :input_files=\u0026gt;1}} 2024-08-20 15:07:45.902 +0000 [INFO] (0001:transaction): embulk-output-bigquery: Get table... …","date":1724221846,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1724221846,"objectID":"eafa1ff5598b90df34ef933c9ce12261","permalink":"http://localhost:1313/todayilearned/post/trocco_and_embluk_catch_up/","publishdate":"2024-08-21T15:30:46+09:00","relpermalink":"/post/trocco_and_embluk_catch_up/","section":"post","summary":"🎢 Summarizing Trocco and Embluk to deepen understanding.","tags":["ETL/ELT","Trocco","Saas","Embluk","OSS"],"title":"Trocco and Embluk Catchup","type":"post"},{"authors":null,"categories":[""],"content":" Why 何かと便利なクジラを使いたい。\nWhat Docker って何が嬉しいの？ Docker コマンド一覧 Setup $ brew install docker インストール $ docker --version バージョン確認 $ docker login ログイン $ docker logout ログアウト Docker File $ docker build . Dockerfileの格納されているディレクトリ上で実行（. はcdの意） $ docker build -t {name} {directory} 名前指定してビルド Docker Image $ docker images イメージ一覧 $ docker pull {image} イメージ取得（eg. DockerHub → DockerImage） $ docker build {directory} イメージ化（DockerFile → DockerImage） $ docker run -it {image} bash イメージ実行（コンテナ起動）　※「pull」「start」を内部的に実行 $ docker run -it --rm {image} bash コンテナ起動後に削除 $ docker run -it -v {host}:{container} {imege} ホストのファイルシステムをコンテナにマウント eg. $ docker run -it -v ~/host/mounted_folder:/new_dir {image} bash docker run -u {UserId}:{UserGroup} ユーザーID、グループ名を指定してコンテナ作成 eg. $ docker run -it -u $(id -u):$(id -g) -v ~/host/mounted_folder:/new_dir {image} bash $ id -u PCのユーザーID確認 $ id -g PCのグループ名確認 $ docker run -p {host_port}:{container_port} ホストのポートをコンテナポートに繋げる eg. $ docker run -it -p 8888:8888 --rm jupyter/datascience-notebook bash $ docker --cpus {# of CPUs} コンテナがアクセスできるCPUI上限確認 $ docker --memory {byte} eg. $ docker run -it --rm --cpus 2 --memory 2g ubuntu bash $ sysctl -n hw.physicalcpu_max 物理コア数 $ sysctl -n hw.logicalcpu_max 論理コア数 $ sysctl hw.memsize メモリ（byte） $ docker rmi {image} イメージ削除 $ docker rmi $(docker images -q) 全イメージ削除 Docker Container $ docker ps -a コンテナ一覧（ps=process status） $ docker inspect {container} コンテナのあらゆる情報確認 eg. $ docker inspect 5f90be76cd31 | grep -i cpu CPU数やメモリ量等確認時 （|grep=抽出 -i=ignore大文字小文字問わず {検索語句} ） $ docker restart {container} コンテナ再起動 $ docker exec -it {container} bash コンテナ実行 $ docker stop {container} コンテナ停止 $ docker stop $(docker ps -q) 全コンテナ停止 $ docker rm {container} コンテナ削除 $ docker system prune、docker rm $(docker ps -q -a) 全コンテナ削除 $ docker run --name {name}{imagename} コンテナ名付け $ docker run -d {imagename} detached mode コンテナ起動後にdetachする（バックグラウンドで動かす） $ docker run --rm {imagename} foreground mode コンテナをExit後に削除する（使い捨てコンテナ用） $ docker commit {imageid/name} {new_imagename(:tag)} コンテナ更新 $ docker tag {new_imagename(:tag)} {target} コンテナ名変更 $ docker push {imagename} コンテナをDockerHubへ Dockerfile HandsOn Overview ローカルやクラウドで、データ分析環境を容易に構築するためのDockerfile\n# Host (Local or AWS/GCP) docker/ ┝ dsenv_build/ │　└ Dockerfile \u0026lt;-ここ作る └ ds_python/ \u0026lt;-任意名 # Docker Containar (root)/ ┝ bin/ ~ ┝ opt/ │　└ anaconda3 └ work/ \u0026lt;-「ds_python」と紐づいている (eg.「test.ipnb」Jupyterlabなどで分析する場所） 分析環境構築 構築（ローカル） ディレクトリ作成 $ mkdir docker $ cd docker Dockerイメージ作成してコンテナ起動 $ docker build . $ docker run -p 8888:8888 -v ~/docker/ds_python:work --name my_lab {image} 構築（クラウド: AWS） SSHでクラウドに接続（セキュアにログイン） $ ssh -i hoge.pem ubuntu@ec2-00-100-200-300.ap-northeast-1.compute.amazonaws.com nb. 予めAWSに（EC2サーバー）インスタンスを作成して、そこにログインしておく。 hoge.pem AWSのアクセス認証鍵で名前は任意。（鍵のあるディレクトリで実行） ec2-{~}.compute.amazonaws.com PublicDNSは都度取得。 クラウド上にDockerCE（ComunicatEdition）をインストール (aws)$ sudo apt-get update (aws)$ sudo apt-get install docker.io (aws)$ docker --version (aws)$ sudo gpasswd -a ubuntu docker #sudo無しでdockerコマンド使用する為 nb. DockerEE（EnterprizeEdition）はガチの奴だから今回はCE Dockerimageをクラウドに送る ※　ネットワークに繋がる状況下か否かでケース分している [case.1] クラウドがネットワークに繋がれるなら SFTPでローカルからクラウドへファイル転送（セキュアに） $ sftp -i hoge.pem ubuntu@ec2-00-100-200-300.ap-northeast-1.compute.amazonaws.com (aws)$ put ~/docker/dsenv_build/Dockerfile home/ubuntu buildcontextを作成して、ファイル移動 $ ssh -i hoge.pem ubuntu@ec2-00-100-200-300.ap-northeast-1.compute.amazonaws.com (aws)$ mkdir dsenv_build (aws)$ mv Dockerfile dsenv_build dockerイメージ作成 (aws)$ cd dsenv_build (aws)~/dsenv_build$ docker build . dockerコンテナ、JupyterLab起動。 (aws)~dsenv_build$ docker run -v ~:/work -p 8888:8888 {imageID} @ec2-00-100-200-300.ap-northeast-1.compute.amazonaws.com:8888 にブラウザーからアクセスする。\n[case.2] クラウドがネットワークに繋がれないなら ローカルでDockerimageをtarで圧縮 $ docker build . $ docker save {imageID} \u0026gt; myimage.tar nb. MacのM1チップ使用時は $ docker build --platform linux/amd64.\nSFTPでローカルからクラウドへファイル転送（セキュアに） $ sftp -i hoge.pem ubuntu@ec2-00-100-200-300.ap-northeast-1.compute.amazonaws.com (aws)$ put ~/docker/myimage.tar home/ubuntu (aws)$ docker load \u0026lt; myimage.tar #tarファイルを解凍 nb. $ docker load で解凍され、同時にイメージが作られる。\nクラウド上でDockerコンテナ起動 (aws)$ docker run -it {imageID} bash Dockerfile作成 FROM ubuntu:latest RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ sudo \\ wget \\ vim WORKDIR /opt RUN wget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh \u0026amp;\u0026amp; \\ sh Anaconda3-2019.10-Linux-x86_64.sh -b -p /opt/anaconda3 \u0026amp;\u0026amp; \\ rm -f Anaconda3-2019.10-Linux-x86_64.sh ENV PATH /opt/anaconda3/bin:$PATH WORKDIR / RUN pip install --upgrade pip CMD [\u0026#34;jupyter\u0026#34;, \u0026#34;lab\u0026#34;, \u0026#34;--ip=0.0.0.0\u0026#34;, \u0026#34;--allow-root\u0026#34;, \u0026#34;--LabApp.token=\u0026#39;\u0026#39;\u0026#34;] nb.\n$ jupyter lab で起動\n--ip 0.0.0.0 ローカルホストの意味\n--allow-root ローカルで実行の為ルート権限（※ 別サーバーでならユーザー指定するべき）\n--LabApp.token トークン指定でセキュリティを高める。（ローカル上の為未指定）\n書き方\nFROM FROM {dockerimage} # OS等々指定 RUN RUN {linux comand} # やりたいこと eg. RUN touch test RUN echo `hello world` \u0026gt; test RUNの他に、COPY、ADDがレイヤーを作成するインストラクション \u0026amp;\u0026amp;: コマンド結合 *- \\: 改行 apt: ubuntuのパッケージ管理(パッケージインストール) RUN apt-get install {package} RUN apt-get update: 最新版取得 (注意) RUN を複数書きすぎてファイルが重く …","date":1724166034,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1724166034,"objectID":"7ca20ec8bd139235b21da7077946f4a6","permalink":"http://localhost:1313/todayilearned/post/docker_commands/","publishdate":"2024-08-21T00:00:34+09:00","relpermalink":"/post/docker_commands/","section":"post","summary":"🐋 Deepen my understanding of Docker's basic commands and mechanisms.","tags":["Docker","CLI","HandsOn"],"title":"Docker Commands","type":"post"},{"authors":null,"categories":[""],"content":" Why SSIA\nWhat Command エディタ起動 $ vim ( $ view Viewモードを使って、ファイルの書き換えミスの保存やアクシデントなどを防ぎたい時に便利！） [V] ビジュアルモード v 複数文字や行を選択できるモード [I] インサートモード i: ファイルに書き込むためのモード a 次の文字からインサートモードに切替え o カーソルの下に空白を入れ、インサートモードに切替え 0 カーソル行に空白を入れ、インサートモードに切替え [:] コマンドモード : 保存 / 終了\n:w 上書き保存 :wq 保存して終了 :q 編集終了 :q! 保存をせずに終了 削除\n:%d: 一括削除（ファイルの中身を空の状態にする） ジャンプ（移動）\n:${数字} 指定した行数に移動 eg. :5 5行目に移動 :10 10行目に移動 :set number 行数を表示 :set nonumber 行数を非表示 コマンドを実行\n:!{$コマンド} eg. :!ls lsコマンド（一覧表示） :!python3 test.py Pythonファイルを実行 :!ruby test.rb Rubyファイルを実行 :!! 前のコマンドを実行 複数ファイルオープン -:e ファイルパス 指定したファイルを開く。:editの略。（eg.:e ~/.vimrc）\nバッファ（=開いたファイルのこと）の一覧から、オープン\n:ls バッファの一覧表示。 :bn 次のバッファを表示。:bnextの略 :bp 次のバッファを表示。:bpreviousの略 :b# 直前まで開いていたバッファを表示。 :b[N] 該当の番号のバッファを表示。番号はファイルを開いた順に振られる。（eg. :b2） 画面分割\n:sp 画面を上下に分割する。:splitの略 :vs 画面を左右に分割する。:vsplitの略 置換\n:s/old/new 該当行の最初の一致箇所を置換 :s/old/new/g 該当行の一致箇所を全置換 :1,10s/old/new/g 1〜10行目の一致箇所を全置換 :%s/old/new/g ファイル全体の一致箇所を全置換 :%s/${検索ワード}/${置換ワード}/g : 一括置換 :%s/${検索ワード}/${置換ワード}/gc: 確認しながら置換 :\u0026#39;\u0026lt;,\u0026#39;\u0026gt;s/{old}/{new}/g [V] visual-modeで選択内 一括置換 cf. Vimの置換コマンドの使い方 Memo on the web vim 文字列置換 基本的な事 Qiita [ ] ノーマルモード [esc]: 基本移動（カーソル移動）\nk ↑ 上に移動 j ↓ 下に移動 h ← 左に移動 l → 右に移動 特殊移動\n0 行頭に移動 ^ / $ インデントの先頭/行末に移動 { / } ひとつ上/下の段落に移動 [[ / ]] ひとつ上/下の空白行に移動 gg / G ファイルの先頭/最後に移動 ctrl + o 移動前に戻る ウィンドゥ間の移動\n\u0026lt;C-w\u0026gt;h カーソルを1つ左のwindowに移動 \u0026lt;C-w\u0026gt;j カーソルを1つ下のwindowに移動 \u0026lt;C-w\u0026gt;k カーソルを1つ上のwindowに移動 \u0026lt;C-w\u0026gt;l カーソルを1つ右のwindowに移動 \u0026lt;C-w\u0026gt;w カーソルを1つ前のwindowに移動 戻る(undo) \u0026amp; 進む(redo)\nu ひとつ前の状態に戻す = その行でインデント位置を自動修正 ctrl + r 直前の操作に進む 削除\nx 1文字削除 dd 1行削除 2dd 2行削除 3dd 3行削除 dw 単語ごとに削除 コピー(ヤンク) \u0026amp; ペースト(プット)\nyy（ヤンク）1行コピー {数字}yy （ヤンク）複数行コピー eg. 2yy 2行コピー 3yy 3行コピー p（プット）カーソルの下にペースト yyp コピペ ddp カット＆ペースト J カーソル行と下の行を連結する 検索 \u0026amp; 置換\n\\${ワード} 検索 n / N 次/前の検索結果に移動 R 置換 ? 逆方向検索 :set is 部分マッチ検索 :set hls 検索マッチする文字強調表示 画面分割\n\u0026lt;C-w\u0026gt;s 画面を上下に分割。ctrlを押しながらwと入力し、その後にsを入力、の意味 \u0026lt;C-w\u0026gt;v 画面を上下に分割。ctrlを押しながらwと入力し、その後にvを入力、の意味 折り畳みの操作方法\nza 現在の折り畳みを開閉（toggle）する zc 現在の折り畳みを閉じる zo 現在の折り畳みを開く zM 全ての折り畳みを閉じる zR 全ての折り畳みを開く Convenience 一括操作（コメントアウト） Ctrl + v 矩形（くけい）選択モードで行をバッと選択 Shift + i コメントアウト文字を入力（e.g. #、--、//…etc.） Esc 押下後にバッと入力されるっ！ cf. vimを使わないviで一括コメントアウト Qiita 【Vim】複数行に一括でコメントアウト Qiita Ref. Vimの基本的なコマンドリスト Qiita Vim幼稚園からVim小学校へ Qiita はじめてのVim 〜 Vimはいいぞ！ゴリラと学ぶVim講座(1) さくらのナレッジ テキストファイルに対してSQLを発行できるツール「q」\u0026amp; vimから使う「vimq.vim」の紹介 Zenn vimtutorで速習Vim 今更ながらvimチートシート Qiita 慣れてきた頃に知りたいVimの便利機能 Zenn Vim ( \u0026amp; ideavim ) キーマップ設定ガイド Qiita 【vimめも】 3. レジスタ Qiita テキストエディタ「vim」入門 envader ","date":1723824123,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723824123,"objectID":"5458da38862d4b4add75e8ee7ce70b61","permalink":"http://localhost:1313/todayilearned/post/vim_commands/","publishdate":"2024-08-17T01:02:03+09:00","relpermalink":"/post/vim_commands/","section":"post","summary":"✅ A list of vim commands for the TextEditor tool, along with various tips.","tags":["TextEditor","IDE","Vim"],"title":"Vim Commands","type":"post"},{"authors":null,"categories":[""],"content":" Why 「タダ(無料)じゃないんだよ！…」お金のかかるお話はどうも苦手ですが、、 しかし、よく扱うからこそ BigQuery の課金の仕組み等をキチンと知っておきたいとそう思う訳で。\nWhat Billing price List（料金表） Computing (分析料金) Billing Fee Free 1. Query(オンデマンド) $7.50/TB 1TB/mm 2. Query(月定額) $2,400/100slots - 3. Query(年定額) $2.040/100slots - 4. BQeditions(Standard) $0.051/slots(h) - 5. BQeditions(Enterprise) $0.076/slots(h) - 6. BQeditions(EnterprisePlus) $0.128/slots(h) - Strage (ストレージ料金) Billing Fee Free 1. Activ (論理) $0.023/GB 10GB/mm 2. Longterm (論理) $0.016/GB 10GB/mm 3. Active (物理) $0.052/GB 10GB/mm 4. Longterm (物理) $0.026/GB 10GB/mm cf. BigQuery の料金 GoogleCloud Slots / Reservation ザックリ買い占めたい人ならSLOT単位でまとめ買い オンデマンド料金モデル スロットと分析容量を明示的に制御不可能 専用 or 自動スケーリングされたクエリ処理容量に対して支払う。 ie. 一時的なバースト容量を備えたプロジェクトごとのスロットの割り当てが適用 アカウントで使用しているスロットの数を確認するには、BigQuery のモニタリング 容量ベースの料金モデル スロットと分析容量を明示的に制御可能 予約するスロット数を明示的に選択。クエリはその容量内で実行され、デプロイされる 1 秒ごとに容量に対して継続的に支払う eg. BigQueryスロットを 2,000個 購入した場合、 集計クエリは、任意の時点で 2,000個の仮想 CPU を使用する状態に制限。 削除するまで 2,000スロットに対して支払う。（この容量は削除するまで保持される） cf. Work with slot reservations (ja) GoogleCloud Understand slots (ja) GoogleCloud Slots and reservations BigQueryのスロットの購入・予約・割り当てをSQLだけで実施する DevelopersIO Ref. BigQuery の料金 GoogleCloud BigQuery エディションの概要 GoogleCloud クエリ費用を管理する GoogleCloud 費用の見積もりと管理 GoogleCloud Google BigQuery の料金体系を解説 電算ｼｽﾃﾑ ","date":1723808107,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723808107,"objectID":"eb16b1effbc2438c182b051fd279dc9b","permalink":"http://localhost:1313/todayilearned/post/bigquery_billing_how_works/","publishdate":"2024-08-16T20:35:07+09:00","relpermalink":"/post/bigquery_billing_how_works/","section":"post","summary":"🔍 Summarize billing mechanism, fee structure, and money-saving tips.","tags":["BigQuery","Data Reliability Engineering"],"title":"BigQuery Billing how works","type":"post"},{"authors":null,"categories":[""],"content":"Why 購入した書籍をPDF化（自炊）して処分・整理しています。 書籍の自炊をする際に使用している道具・アプリの紹介等まとめる。\nWhat [自炊] 使用ツール\nとじたくん: 本の背表紙を溶解 裁断機: 溶解したノリを裁断 ScanSnap iX1500: スキャナー Booklover: PDFデータ管理アプリ 自炊のやり方\ncf. だすまんちゃん直伝！本や教科書を裁断・自炊してPDF化する方法 様々な課題 どこで管理する？\ncf. 無期限にデータを保存するならドコ？オンラインストレージ比較 著作権侵害大丈夫？\n私的利用の範囲 cf. 本の自炊iCloudなどで保存する場合の著作権侵害について。本を購入し、… Ref. だすまんちゃん直伝！本や教科書を裁断・自炊してPDF化する方法 無期限にデータを保存するならドコ？オンラインストレージ比較 本の自炊（PDF化）〜管理方法(iPhone, iPadユーザー向け) jisui 本の自炊iCloudなどで保存する場合の著作権侵害について。本を購入し、… Yahoo!知恵袋 ","date":1723801317,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723801317,"objectID":"0d3e276f4452a871e13a48f25498d479","permalink":"http://localhost:1313/todayilearned/post/other/jisui_keep_book_data/","publishdate":"2024-08-16T18:41:57+09:00","relpermalink":"/post/other/jisui_keep_book_data/","section":"post","summary":"📚 Converting cut-up books into digital data. Also, an article on how to cut books and store the data.","tags":["KnowHow","Jisui","Recycle"],"title":"Bookread How to Jisui ~自炊~","type":"post"},{"authors":null,"categories":[""],"content":" Why 普段ブラウザー（e.g. Google Chrome）を使用していて思うことがある。\nよく使うWebアプリケーションをタブ分割ではなく、ネイティブアプリケーションのように使用したいと。 せめてデスクトップ上だけでも。。\nWhat Google Chromeのデフォルト機能を用いて（エセ）ネイティブアプリをDockに追加していく。 PWAの技術とはまた違う。 How 想定環境\nGoogle Chrome（2024/08現在の最新版） MacBook (Chromeが動けばどのマシンでもいけると思う） 手順\nアプリケーション化 Google Chrome で任意のWebアプリケーションにアクセス。 [≡]右上設定ファイルが畳まれているボタン \u0026gt; [Save and Share] \u0026gt; [Install page as App…] \u0026gt; [Install] 格納先（e.g.）：$HOME/Applications/Chrome Apps/[page].app アイコン変更（任意） 作成された[page].app を右クリック \u0026gt; [Show Package Contents] Contents/Resources/app.icns を同名、同拡張子で置換。 いい感じにアイコン作りたい時は以下が便利 .png ↔︎.icns なら cloudconvert 何でもできる なら BANNER KOUBOU Ref. ","date":1722668658,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1722668658,"objectID":"ab45411a18b38a2a8b2b4d4646ec7227","permalink":"http://localhost:1313/todayilearned/post/macbook_looks_like_native_application/","publishdate":"2024-08-03T16:04:18+09:00","relpermalink":"/post/macbook_looks_like_native_application/","section":"post","summary":"🍏 A little trick to use it like a native laptop application.","tags":["macOS","Browser","Chrome","KnowHow"],"title":"MacBook Looks like native application","type":"post"},{"authors":null,"categories":[""],"content":"Why 別に知らなくても生きていけるが、知っていると楽しいよ！ってことらしいので。\n（適時増やしていく）\nWhat cf. 用語調べる e-Words IT用語辞典 Computing Abbreviations 省略形調べる Acronym Finder Top Acronyms and Abbreviations Dictionary 略語 コーディングして、変数名決めたい時。 Readmeにドキュメント残したり読んだりしている時。 GitHubでプルリク出してコードレビューしたり、されたりラジ...ry よく見る\nPR :(pull request) プルリク SSIA :(subject says it all) 掲載(掲題)の通り LGTM :(looks good to me) いい感じ PTAL :(please take another look) 再度ご確認ください NB/N.B. :(nota bene / note well) 特に注意、備考 i.e. :(id est / that is) すなわち c.f./cf. :(confer / compare) ○○を参照 e.g. :(exempli gratia / for example) たとえば ex :(extra) 特別な、元◯◯ tldr/tl;dr/tl/dr :(Too long, didn’t read) 長い文を読みたくない人向けの要約 GCP:(Google Cloud Platform) GKE :(Google Kubernetes Engine) TF :(Terraform / Tensol Flow) K8S :(Kubernetes) gr8 :(Great) LOL :(Laugh out loud) HTH :(Hope this help) コミュニケーション（Slack, Gmail…etc.）\nRE: :(regarding、reply) 返信: R.S.V.P :(respondez s’ il vous plait) ご返信ください N.A. :(not available / not applicable) 該当なし/使用不可 Pls :(Please) ASAP :(as soon as possible) なるはやで FYI :(for your information) ご参考までに FYIG :(for your information and guidance) 情報および指針として、ご参考までに IMO :(in my opinion) 私の意見では EOB :(end of business day) 終業時間までに COD :(close of business）終業時間までに IAC :(in any case) とにかく/いずれにしても WFH :(work from home) 在宅勤務 OOO :(out of office) 不在 AFK :(away from keyboard) 離席します ⌨️ GJ :(Good job) CTN :(Can’t talk now) J/K :(Just kidding) w/e :(Whatever) 経営層\nROI :() 1~4Q :(quoter) 四半期決算 目標管理フレームワーク OKR :(Objectives and Key Results) KPT :() MBO :() KPI/KGI :() PDCA :() マーケター(Web、SNS…etc)\nSEM :() SEO :() PPC :() CV :(conversion) 獲得数 PV :(pageview) 閲覧数 SS :(session) アクセス数 IMP :(impression) 表示回数 CVR :() CPA :() CTA :() ASL :(Age/Sex/Location) 略語（学術的種類） 用語 定義 例 アクロニム 複数の単語の頭文字を組み合わせた略語で、通常はそのまま単語のように発音される。 NATO（North Atlantic Treaty Organization） イニシャリズム アクロニムと似ているが、頭文字を一文字ずつ発音する略語。 FBI（Federal Bureau of Investigation） バクロニム 元の略語に対して後から意味や言葉を当てはめたもので、後付けで作られた言葉の意味がある。 SOS（“Save Our Souls” とされるが、元は単に無線信号） DAI語★　日本のタレントが番組で披露してから、主に日本の若い世代に流行。(知らんけど) OYKK (「俺の嫁北川景子」) cf. 英語の略語、アクロニム＆イニシャリズム＋バクロニム おまけ cf. 急に仕事で英語を使うことになった社会人に贈るまとめ(便利ツール/コンテンツ) Qiita Ref. 英語のコメントや issue で頻出する略語の意味 (FYI, AFAIK, …) Qiita 【変数】英単語の略し方や省略形を検索する【命名】 Qiita 商業略語表（1/3） PDF www.englishgrammarhere.com “FYI”や“TBA”って？ビジネスシーンに登場する英語の便利な略語 OKRとは？Googleやメルカリも導入する目標管理手法を解説 【初心者向け】1Q・2Q・3Q・4Qとは？四半期決算の発表日の調べ方を解説 やさしい投資家の教科書 【ビジネス用語】FYIやTBDの略語から英語の専門用語までご紹介 Berlits “FYI”や“TBA”って？ビジネスシーンに登場する英語の便利な略語 KotsuKotsu ARCHIVED: What do BTW, FAQ, FYI, IMHO, RTFM, and other acronyms mean? University Information ","date":1722311335,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1722311335,"objectID":"b70523ac2d0b99aaaabb766c48594c68","permalink":"http://localhost:1313/todayilearned/post/english_abbreviations/","publishdate":"2024-07-30T12:48:55+09:00","relpermalink":"/post/english_abbreviations/","section":"post","summary":"🗽 About abbreviations. They also appear in business and professional situations.","tags":["English","GitHub","Slack"],"title":"English Abbreviations","type":"post"},{"authors":null,"categories":[""],"content":" Why SSIA\nWhat dbt とは What is dbt? dbt\ndbtとは data build tool の略で、データ統合を行う際のプロセスであるELT(抽出, 変換, 格納)のうち Transform(変換) の役割を担うツールです。 Transformのプロセスでは一般的にデータウェアハウスなどに抽出したデータを下流の分析ツールやデータベースで利用できる形式に変換・加工する処理を行います。 dbtはこの工程で役に立つ様々な機能を提供してくれます。 ref\nOfficial https://www.getdbt.com/ https://docs.getdbt.com/ https://github.com/dbt-labs/ Community 🇯🇵 dbt Developer Group(DDG) Tokyo - Japan dbt User Group https://dbt-ug.tokyo/ dbt-tokyoはdbtの日本におけるdbtの普及と、 dbtプロダクトへの貢献を目的に活動をしています。 Production [dbt] └ 1.[dbt-core] \u0026lt;-Free └ 2.[dbt Cloud] └ 1.[Developer] \u0026lt;-Free └ 2.[Team] └ 3.[Enterprise] dbt-core dbt Cloud WebUIを用いて開発ができる！ cf. dbt Pricing Plans dbt Chapter 02 dbt Cloudのセットアップ Zenn Hands On（dbt-core） 詳細なとは参考に乗っているので、この辺を読んで手を動かした部分を掻い摘む。 dbt 入門 Zenn dbtで始めるデータパイプライン構築〜入門から実践〜 Zenn dbtハンズオン Zenn 導入から実行まで [STEP1] ✴️ dbt 導入\n# 作業場用意 $ mkdir -p sandbox/dbt_training | cd dbt_training # venv仮想環境を用意 $ python3 -m venv venv $ source venv/bin/activate # 仮想環境実行 (venv)$ pip install --upgrade pip # pip更新 (venv)$ pip install dbt-postgres # dbtインストール (venv)$ deactivate # 仮想環境停止 $ # ローカル環境に戻る # dbtに必要なものを準備 $ source venv/bin/activate # 再度仮想環境実行 (venv)$ dbt --version # dbt環境が手元にあるか確認 (venv)$ mkdir \\ # 必要なディレクトリを準備 models \\ analysis \\ tests \\ seeds \\ macros \\ snapshots \\ target　(venv)$ touch dbt_project.yml　# dbt設定ファイルを作成（＊１: 後述のYAMLファイル） *1 --- name: \u0026#39;dbt_training\u0026#39; config-version: 2 version: \u0026#39;1.0.0\u0026#39; profile: \u0026#39;dbt_training_dw\u0026#39; model-paths: [\u0026#34;models\u0026#34;] analysis-paths: [\u0026#34;analysis\u0026#34;] test-paths: [\u0026#34;tests\u0026#34;] seed-paths: [\u0026#34;seeds\u0026#34;] macro-paths: [\u0026#34;macros\u0026#34;] snapshot-paths: [\u0026#34;snapshots\u0026#34;] target-path: \u0026#34;target\u0026#34; clean-targets: [target, dbt_packages] models: dbt_training: example: YAMLファイルの中身の詳細説明はここがわかりやすい. # 上位階層に不可視ディレクトリ作成 $ mkdir .dbt | cd .dbt $ touch profiles.yml # DWH接続ファイルを作成（＊２: 後述のYAMLファイル） *2 --- dbt_training_dw: target: dev outputs: dev: type: postgres host: localhost user: admin password: admin port: 5432 dbname: postgres schema: public threads: 1 keepalives_idle: 0 connect_timeout: 10 上記は「PostgreSQL」に接続させる設定ファイル（cf.公式）。 dbt は DWH の接続設定を ~/.dbt/profiles.yml に書く。 ~/.dbt/profiles.yml は各DWH毎にプロファイルを書く。 選定するDWHの種類(PostgreSQL, BigQuery, Snowflake …etc.) 毎にアダプターがあり、プロファイルはアダプターごとに設定の書き方が異なる. [STEP2] 🐘 データベース（PostgreSQL）設定\n# 再び仮装環境内に入ってDB（今回はPostgreSQL）を用意 (venv)$ touch docker-compose.yml | vi docker-compose.yml --- version: \u0026#39;3\u0026#39; services: postgres: image: postgres:latest restart: always ports: - 5432:5432 environment: POSTGRES_USER: admin POSTGRES_PASSWORD: admin volumes: - ./postgres:/var/lib/postgresql/data # Dockerインストールされているか確認 (venv)$　docker --version # gemを新規で導入するときには、まず以下のコマンドを実行 # cf. https://qiita.com/KenAra/items/f1976caa69468323c29d Qiita (venv)$ docker-compose build (venv)$ docker-compose up -d d # Docker起動 (venv)$ docker-compose stop # Docker停止 もしここでDockerでつまいづいたら🐋 コマンド参照 [STEP3] ✴️ dbt 実行\n(venv)$ dbt run - ※モデル作成していないから, 「`WARNING`」出ているが気にせず◎ - 逆に実行後 `logs/` ディレクトリができていることを確認。 モデリング [Step1] 🐘 データソースの準備\n# PostgreSQLログイン (venv)$ psql -U postgres postgres=# # データベース作成 postgres=# \\d # DB一覧確認 postgres=# CREATE DATABASE dbt_training; # # テーブル作成 postgres=# \\cd dbt_training # 作成したDBに移動 dbt_training=# CREATE SCHEMA raw; # スキーマ作成 dbt_training=# \\du # ロール一覧確認 List of schemas　Name | Owner ------------+---------- public | postgres raw | postgres (2 rows) # 各種テーブル作成 dbt_training=# -- 従業員テーブル CREATE TABLE \u0026#34;dbt_training\u0026#34;.\u0026#34;raw\u0026#34;.\u0026#34;employees\u0026#34; ( \u0026#34;employee_id\u0026#34; varchar(256), \u0026#34;first_name\u0026#34; varchar(256), \u0026#34;last_name\u0026#34; varchar(256), \u0026#34;email\u0026#34; varchar(256), \u0026#34;job_id\u0026#34; varchar(256), \u0026#34;loaded_at\u0026#34; timestamp ); dbt_training=# -- お仕事テーブル CREATE TABLE \u0026#34;dbt_training\u0026#34;.\u0026#34;raw\u0026#34;.\u0026#34;jobs\u0026#34; ( \u0026#34;job_id\u0026#34; varchar(256), \u0026#34;job_title\u0026#34; varchar(256), \u0026#34;min_salary\u0026#34; INTEGER, \u0026#34;max_salary\u0026#34; INTEGER, \u0026#34;loaded_at\u0026#34; timestamp ); dbt_training=# \\dt myschema.* # スキーマ内テーブル一覧確認 # データ格納 dbt_training=# -- 従業員テーブルへデータをINSERT INSERT INTO \u0026#34;dbt_training\u0026#34;.\u0026#34;raw\u0026#34;.\u0026#34;employees\u0026#34; VALUES (\u0026#39;101\u0026#39;,\u0026#39;taro\u0026#39;,\u0026#39;yamada\u0026#39;,\u0026#39;yamada@example.com\u0026#39;,\u0026#39;11\u0026#39;,\u0026#39;2022-03-16\u0026#39;), (\u0026#39;102\u0026#39;,\u0026#39;ziro\u0026#39;,\u0026#39;sato\u0026#39;,\u0026#39;satou@example.com\u0026#39;,\u0026#39;11\u0026#39;,\u0026#39;2022-03-16\u0026#39;) ; dbt_training=# -- お仕事テーブルへデータをINSERT INSERT INTO \u0026#34;dbt_training\u0026#34;.\u0026#34;raw\u0026#34;.\u0026#34;jobs\u0026#34; VALUES (\u0026#39;11\u0026#39;,\u0026#39;datascientist\u0026#39;,6000000,12000000,\u0026#39;2022-03-16\u0026#39;), (\u0026#39;12\u0026#39;,\u0026#39;dataengineer\u0026#39;,5000000,10000000,\u0026#39;2022-03-16\u0026#39;) ; # PostgreSQLログアウト dbt_training=# \\q postgres=# \\q cf. 🐘 PostgreSQL(コマンドリファレンス) [Step2] ✴️ dbt モデル作成\n(venv)$ touch models/employee_names.sql select \u0026#34;employee_id\u0026#34; , concat(\u0026#34;first_name\u0026#34;, \u0026#39; \u0026#39;, \u0026#34;last_name\u0026#34;) as full_name from \u0026#34;dbt_training\u0026#34;.\u0026#34;raw\u0026#34;.\u0026#34;employees\u0026#34; cf. SQL models dbt # モデルを作成後に、dbt実行 (venv)$ dbt run …省略… 1 of 1 OK created view model public.employee_names.............................. [CREATE VIEW in 0.08s] 最終的に 1 of 1 OK created view model と表示されればOK。 ココでは、デフォのviewのモデルが作成される。 PostgreSQL の中に employee_names ビューが生成されているはず。 [Step3] ✴️ dbt モデル作成（様々なモデル：マテリアライゼーション編）\nMaterializations dbt\n種類 モデル データ規模 説明 view Viewで作成 - 未指定だとデフォ。高速モデル構築（データ移動が発生しないから）。 でも view \u0026lt; table でクエリが遅い。初期構築に向いている。 table Tableで作成 小規模 実行($ dbt run)の度にデータ入れ直す. その為小規模データに向いている（大規模データだと都度入れ直しはコストや処理時間面で問題）。参照回数の多い、アドホックな分析や集計後のモデリングに適している( view \u0026lt; tablea でクエリ速い為) …","date":1722274255,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1722274255,"objectID":"714d8f52995c28b001f2244e090bdbe5","permalink":"http://localhost:1313/todayilearned/post/dbt_seeking_basic_understanding/","publishdate":"2024-07-30T02:30:55+09:00","relpermalink":"/post/dbt_seeking_basic_understanding/","section":"post","summary":"✴️ Wanna deepen my understanding by using my hands and referring to reference books.","tags":["dbt","ETL/ELT","HandsOn","Data Reliability Engineering"],"title":"dbt Seeking out basic understanding","type":"post"},{"authors":null,"categories":[""],"content":" 「データサイエンス100本ノック（構造化データ加工編）」 数年前に遊んだノックを再校兼ねて...⚾️ 当時のものだからクエリがいけてないけど良しなに 一般社団法人データサイエンティスト協会 https://github.com/The-Japan-DataScientist-Society/100knocks-preprocess cf. データサイエンス初学者のための実践的な学習環境 「データサイエンス100本ノック（構造化データ加工編）」をGitHubに無料公開 Digital PR Platform 構築イメージ cf. Digital PR Platform 使用テーブルの確認 select　*　from( select \u0026#39;category\u0026#39; as tb_name , (select count(*) from `project.100knocks.category`) as reco , count(*) as col from `project.100knocks.INFORMATION_SCHEMA.COLUMNS` where table_name = \u0026#39;category\u0026#39; union all select \u0026#39;customer\u0026#39; as tb_name , (select count(*) from `project.100knocks.customer`) as reco , count(*) as col from `project.100knocks.INFORMATION_SCHEMA.COLUMNS` where table_name = \u0026#39;customer\u0026#39; union all select \u0026#39;geocode\u0026#39; as tb_name , (select count(*) from `project.100knocks.geocode`) as reco , count(*) as col from `project.100knocks.INFORMATION_SCHEMA.COLUMNS` where table_name = \u0026#39;geocode\u0026#39; union all select \u0026#39;product\u0026#39; as tb_name , (select count(*) from `project.100knocks.product`) as reco , count(*) as col from `project.100knocks.INFORMATION_SCHEMA.COLUMNS` where table_name = \u0026#39;product\u0026#39; union all select \u0026#39;receipt\u0026#39; as tb_name , (select count(*) from `project.100knocks.receipt`) as reco , count(*) as col from `project.100knocks.INFORMATION_SCHEMA.COLUMNS` where table_name = \u0026#39;receipt\u0026#39; union all select \u0026#39;store\u0026#39; as tb_name , (select count(*) from `project.100knocks.store`) as reco , count(*) as col from `project.100knocks.INFORMATION_SCHEMA.COLUMNS` where table_name = \u0026#39;store\u0026#39; ) order by reco desc ; 100本ノック(SQL編) | S-001 ★ レシート明細テーブル(receipt)から全項目を10件抽出し、どのようなデータを保有して いるか目視で確認せよ。\nselect * from `100knocks.receipt` limit 10; | S-002 ★ レシート明細のテーブル(receipt)から売上日(sales_ymd)、顧客ID (customer_id)、商品コード(product_cd)、売上金額(amount)の順に列を指定し、 10件表示させよ。\nselect sales_ymd , customer_id , product_cd , amount from `100knocks.receipt` limit 10 ; |S-003 ★ レシート明細のテーブル(receipt)から売上日(sales_ymd)、顧客ID (customer_id)、商品コード(product_cd)、売上金額(amount)の順に列を指定し、10件表示させよ。ただし、sales_ymdはsales_dateに項目名を変更しながら抽出するこ と。\nselect sales_ymd as sales_date , customer_id , product_cd , amount from `100knocks.receipt` limit 10 ; | S-004 ★ レシート明細のテーブル(receipt)から売上日(sales_ymd)、顧客ID (customer_id)、商品コード(product_cd)、売上金額(amount)の順に列を指定し、 以下の条件を満たすデータを抽出せよ。\n顧客ID（customer_id）が\u0026#34;CS018205000001\u0026#34; select sales_ymd , customer_id , product_cd , amount from `100knocks.receipt` where customer_id = \u0026#39;CS018205000001\u0026#39; ; | S-005 ★ レシート明細のテーブル(receipt)から売上日(sales_ymd)、顧客ID (customer_id)、商品コード(product_cd)、売上金額(amount)の順に列を指定し、 以下の条件を満たすデータを抽出せよ。\n顧客ID（customer_id）が\u0026#34;CS018205000001\u0026#34; 売上金額（amount）が1,000以上 select sales_ymd , customer_id , product_cd , amount from `100knocks.receipt` where customer_id = \u0026#39;CS018205000001\u0026#39; and 1000 \u0026lt;= amount ; | S-006 ★ レシート明細テーブル(receipt)から売上日(sales_ymd)、顧客ID(customer_id)、 商品コード(product_cd)、売上数量(quantity)、売上金額(amount)の順に列を指定 し、以下の条件を満たすデータを抽出せよ。\n顧客ID（customer_id）が\u0026#34;CS018205000001\u0026#34; 売上金額（amount）が1,000以上または売上数量（quantity）が5以上 select sales_ymd , customer_id , product_cd , quantity , amount from `100knocks.receipt` where customer_id = \u0026#39;CS018205000001\u0026#39; and (1000 \u0026lt;= amount or 5 \u0026lt;= quantity) ; | S-007 ★ レシート明細のテーブル(receipt)から売上日(sales_ymd)、顧客ID (customer_id)、商品コード(product_cd)、売上金額(amount)の順に列を指定し、以下の条件を満たすデータを抽出せよ。\n顧客ID（customer_id）が\u0026#34;CS018205000001\u0026#34; 売上金額（amount）が1,000以上2,000以下 select sales_ymd , customer_id , product_cd , amount from `100knocks.receipt` where customer_id = \u0026#39;CS018205000001\u0026#39; and amount between 1000 and 2000 ; | S-008 ★ レシート明細テーブル(receipt)から売上日(sales_ymd)、顧客ID(customer_id)、 商品コード(product_cd)、売上金額(amount)の順に列を指定し、以下の条件を満た すデータを抽出せよ。\n顧客ID（customer_id）が\u0026#34;CS018205000001\u0026#34; 商品コード（product_cd）が\u0026#34;P071401019\u0026#34;以外 select sales_ymd , customer_id , product_cd , amount from `100knocks.receipt` where customer_id = \u0026#39;CS018205000001\u0026#39; and product_cd \u0026lt;\u0026gt; \u0026#39;P071401019\u0026#39; ; | S-009 ★ 以下の処理において、出力結果を変えずにORをANDに書き換えよ。\nselect * from store where not (prefecture_cd = \u0026#39;13\u0026#39; or floor_area \u0026gt; 900)\nselect * from `100knocks.store` where prefecture_cd \u0026lt;\u0026gt; 13 and floor_area \u0026lt; 900 ; | S-010 ★ 店舗テーブル(store)から、店舗コード(store_cd)が\u0026#34;S14\u0026#34;で始まるものだけ全項目抽 出し、10件だけ表示せよ。\nselect * from `100knocks.store` where store_cd like \u0026#39;S14%\u0026#39; -- where regexp_contains(store_cd, r\u0026#39;^S14\u0026#39;) #別解 limit 10 ; | S-011 ★ 顧客テーブル(customer)から顧客ID(customer_id)の末尾が1のものだけ全項目抽出 し、10件だけ表示せよ。\nselect * from `100knocks.customer` where regexp_contains(customer_id, r\u0026#39;1$\u0026#39;) limit 10 ; | S-012 ★ 店舗テーブル(store)から横浜市の店舗だけ全項目表示せよ。\nselect * from `100knocks.store` where address like \u0026#39;%横浜市%\u0026#39; ; | S-013 ★★ 顧客テーブル(customer)から、ステータスコード(status_cd)の先頭がアルファベッ トのA〜Fで始まるデータを全項目抽出し、10件だけ表示せよ。\nselect * from `100knocks.customer` where regexp_contains(customer_id, r\u0026#39;^(A|B|C|D|E|F)\u0026#39;) -- regexp_contains(customer_id, r\u0026#39;^(A-F)\u0026#39;) -- 別解 -- customer_id like \u0026#39;A%\u0026#39; -- or customer_id like \u0026#39;B%\u0026#39; -- or customer_id like \u0026#39;C%\u0026#39; -- or customer_id like \u0026#39;D%\u0026#39; -- or customer_id like \u0026#39;E%\u0026#39; -- or customer_id like \u0026#39;F%\u0026#39; -- 別解 limit 10 ; Cf. BigQueryでLIKE文の複数条件指定をORから正規表現に直す Qiita\n| S-014 ★★ 顧客テーブル(customer)から、ステータスコード(status_cd)の末尾が数字の1〜9で 終わるデータを全項目抽出し、10件だけ表示せよ。\nselect * from `100knocks.customer` where …","date":1721935418,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721935418,"objectID":"653721b897ba0218675e911dc00ad03e","permalink":"http://localhost:1313/todayilearned/post/bigquery_structured_data_processing_100/","publishdate":"2024-07-26T04:23:38+09:00","relpermalink":"/post/bigquery_structured_data_processing_100/","section":"post","summary":"🔍 I tried using SQL to learn the basics of data science.","tags":["BigQuery","SQL","HandsOn","Data Analytics"],"title":"BigQuery Structured Data Processing 100","type":"post"},{"authors":null,"categories":[""],"content":" Why SSIA\nWhat [Identicon] Identiconとは\n↑こんなやつ(GitHubからの最初の贈り物) ie. 自分のIdenticonにアクセス https://github.com/identicons/\u0026lt;username\u0026gt;.png cf.\nGitHubのIdenticonの生成と取得 Qiita Identicons! GitHub Blog Identicon Wikipedia [Octocat] https://myoctocat.com/ https://github.com/octocat\ncf. オリジナルOctocatを作ろう！ [emoji] :octocat: :atom: :electron: https://emojipedia.org/github https://emojis.sh/ cf. Emojis [Dummy Account] $ git commit をするとコミットログに自分のアドレスが見れてしまう。\n設定すると、以下のような秘匿性の高いアドレスに変更することができる。\nUSERNAME@users.noreply.github.com cf.\nコミットメールアドレスを設定する GitHub Doc GitHub でダミーのメールアドレスを使用する Qiita 【Git】メールアドレスをnoreplyに設定する Qiita [Powerful README] 色々おしゃれにしちゃおう。\n[Offline Event] cf. GitHub Universe GitHub GitHub Universe 2023 現地会場!! GitHub カンファレンスレポート at サンフランシスコ Qiitai 【週刊オルターブース】『GitHub Universe 2023』現地参加のブログ祭り編 週間オルターブース [Festival] 直接的ではないが、GitHubに係るその他のお祭り\ncf. Hacktoberfest プログラマのためのお祭り、”ハックトーバーフェスト”が開催中！ Medium [ATOM (TextEditor)] 駆け出しエンジニアの時にお世話になった製品 m(_)m (2023/5/4: ｻ終)\nhttps://atom-editor.cc/ community\nhttps://atom-community.github.io/ https://github.com/atom-community cf.\nAtom (text editor) Wikipedia GitHub 製エディタ Atom リファレンス Qiita Atomエディタが開発終了へ！ 他のエディタに移行するならどれがオススメか？ Qbook Pulsar\nsrc. https://pulsar-edit.dev/ https://github.com/pulsar-edit Package install. $ brew install --cask pulsar cf. Atom後継のPulsarを使う Zenn memo 有志団体が開発 [Dangit, Git!?!（処方箋）] https://dangitgit.com/ja https://x.com/jalva_dev/status/1821742876080124039?s=46\u0026amp;t=RSRnjQg5szbGQ19iUHwMlg [🧾レシート] https://gitreceipt.vercel.app/\n[Uithub] https://uithub.com/ https://uithub.com/openapi.html Uithubは、GitHubリポジトリを使ってプロジェクトのドキュメントを自動生成するサービス e.g. https://github.com/gcpug/nouhau -\u0026gt; https://uithub.com/gcpug/nouhau ","date":1721801293,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721801293,"objectID":"36c7da3cde8662e458ee999598337a4c","permalink":"http://localhost:1313/todayilearned/post/github_one_thing_or_another/","publishdate":"2024-07-24T15:08:13+09:00","relpermalink":"/post/github_one_thing_or_another/","section":"post","summary":"📍 Various things to make your GitHub life more enjoyable.","tags":["GitHub"],"title":"GitHub One thing or another (ｱﾚｺﾚ)","type":"post"},{"authors":null,"categories":[""],"content":" Why SSIA\nWhat Slackのマークアップ 書式一覧 Slack（Markdown） 用途 *太字* _イタリック_ ~取り消し~ \u0026gt; 引用 \u0026gt;\u0026gt;\u0026gt; 引用ブロック \u0026#39;コード\u0026#39; ※[`]バッククオート ```コードブロック``` 1. 手順 - リスト + リスト * リスト ※ネストはtab :emoji: e.g. :bow: [リンク](url) @username メンション指定 #channelname チェンネル指定 cf. メッセージの書式設定 Slack\nMarkdownじゃないよ ChatGPTﾆｷ曰く\nSlack のメッセージ書式は、Markdown と似ていますが、独自の仕様を持っています。\nまた、GFM（GitHub Flavored Markdown）とは異なります。GFM は GitHub が使用する Markdown 拡張であり、標準的な Markdown に幾つかの追加機能があります。\nしたがって Slack の書式設定は独自のものであり、Markdown や GFM とは異なります。しかし、基本的なアイデア（太字、斜体、引用、コードブロックなど）は共通しています。Markdown や GFM で可能な高度な機能（例えば、表やチェックリストなど）は Slack ではサポートされていません。\nVariety emoji Slackmojis 絵文字 ジェネレーター MEGAMOJI カスタム絵文字をサクッと作れる🐱 Slack API Block Kit cf. Slack で UI を構築するためのフレームワーク「Block Kit」の基本を確認してみた classmethod 外部機能 Google Extension SlackUtils Ref. メッセージの書式設定 Slack テキストコミュニケーションで心掛けること Slack Slack の書式設定でメッセージをもっと伝わりやすく Slack Slack のメッセージ記法と Markdown を比較してみる Qiita Slackの文章が劇的に綺麗になる! markdown記法を利用しよう samurai ","date":1721726237,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721726237,"objectID":"b2ed8385d3df92ad21a5c3dd39742cd4","permalink":"http://localhost:1313/todayilearned/post/slack_writing_etiquette/","publishdate":"2024-07-23T18:17:17+09:00","relpermalink":"/post/slack_writing_etiquette/","section":"post","summary":"💬 Learn writing etiquette using Slack's unique notation.","tags":["Slack","Markdown","GFM"],"title":"Slack Writing etiquette","type":"post"},{"authors":null,"categories":[""],"content":" Why 個人や、グループ開発で様々なシーン 差分確認 の場面がある。 その中で、 いち早く問題特定 ができるようになりたい。 バグ修正時やペアプログラミングの際、はたまたコードレビュー時に鋭くかつ適切に理解ができる効果を期待したいため。\nHow [App] SourceGraph https://github.com/sourcegraph\nいち早く問題特定するためには、何といってもコード理解が必須である。 いちいち差分を手作業で見たり、そもそも修正箇所を特定してから差分比較するというときにはやはり、リポジトリ内での検索力が問われる。\nそこで、「SourceGraph」なるツールがあることを知った。 ref\ncf. Global Code Time Report Software [WebUI] GitHub Code Search (β版) https://github.com/features/code-search/ Try it now https://github.com/search?type=code\u0026amp;auto_enroll=true Watch video https://www.youtube.com/watch?v=ujVY8xqkflQ 検索窓に「repo:{your/repository/name} {your/file.name}」 会社の凄い人は、こんな風に検索をかけて調べていた。 ※最近では専らコレばかり使用している。\n[WebUI] URL URLを加工してBrowser画面上で比較（※便宜的にURLノード別に改行している）\nhttps://github.com /[ユーザー（組織）名] /[リポジトリ名] /compare /[Old]...[New] コミット間 GitHubのリポジトリ頁へ 「🕐 Commits」ボタンをクリック コミット一覧より比較したい世代のコミットIDを選択 https://github.com/[ユーザー（組織）名]/[リポジトリ名]/compare/コミットハッシュ値old...コミットハッシュ値new e.g. https://github.com/numpy/numpy/compare/b01a473...4d29079 ブランチ間 リポジトリのトップ頁から「Compare」ボタンをクリック ブランチの比較頁へ遷移 e.g. https://github.com/numpy/numpy/compare/master...maintenance/1.1.x 日付間 n日前 e.g. https://github.com/numpy/numpy/compare/master@{7.day.ago}...master 指定日 e.g. https://github.com/numpy/numpy/compare/master@{2020-12-01}...master [WebUI] Link Share GitHubのWebUI上で、コードやページに対して共有リンクを発行することができる。\n修正したい箇所や共有したい箇所に対して、右クリックで発行してシェアすることができる。\n[CLI] $ git diff $ git diff コミット間 今回コミットと直前 $ git diff HEAD^ 指定コミット同士 $ git diff 変更前のSHA..変更後のSHA ブランチ間 指定ブランチ間 $ git diff ブランチA..ブランチB [CLI] $ git grep 第13話 どのコミットでバグが入ったかgit blameで調べてみよう【連載】マンガでわかるGit ～コマンド編～\n[CLI] $ git blame コードを書いたのが誰かわかる超便利コマンド「git blame」の使い方を解説\n[CLI] shell # GitHub CLI や Git コマンドで対象レポをローカルにクローン $ gh clone https://github.com/username/repository.git | cd repository $ git clone https://github.com/username/repository.git | cd repository $ grep -r \u0026#34;特定の文字列\u0026#34; . Ref. コミットを比較する GitHub Doc GitHubの差分確認方法(ブランチ、コミット、日付) Qiita GitHub – コミット間・ブランチ間の差分(Diff)の確認方法・見方 Howpon 忘れやすい人のための git diff チートシート Qiita gitのdiff, status, logを極限までコンパクト化＋便利化する Qiita ソースコードgrep地獄を助けるSourceGraph Qiita GitHubのプルリクエストで差分が更新されない時 Qiita gitでmergeしても差分が反映されない Stackoverflow ","date":1721675601,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721675601,"objectID":"76b002aadcb652cf215443af13cdad72","permalink":"http://localhost:1313/todayilearned/post/github_how_to_check_difference/","publishdate":"2024-07-23T04:13:21+09:00","relpermalink":"/post/github_how_to_check_difference/","section":"post","summary":"📍 Methods and tools to capture code fixes faster and more accurately.","tags":["GitHub","KnowHow","Git","SourceGraph"],"title":"GitHub How to check difference code","type":"post"},{"authors":null,"categories":[""],"content":" Why SSIA\nWhat CTEの定義 文脈によって若干の違いがあるのものの `CTE` と `CTEs` は基本的に同じ概念を指している（細かいｶﾖ!）。 CTE（Common Table Expression） 定義: 一時的に名前を付けて結果セットを定義し、それを後続のクエリで参照するSQL構文。 単一のCTEを使用する場合、単数形のCTEという用語が使用される。 -- 単一のCTE with SingleCTE as ( select column1, column2 from `dataset.table` where condition ) select column1, count(*) as count from SingleCTE group by column1 ; CTEs（Common Table Expressions） 定義: 複数のCTEを組み合わせて使用する場合、それらを総称してCTEsと言う。 -- 複数のCTE（CTEs） with FirstCTE as ( select column1, column2 from `dataset.table` where condition1 ) , SecondCTE as ( select column1, sum(column3) as sum_column3 from FirstCTE where condition2 group by column1 ) , ThirdCTE as ( select column1, sum_column3 from SecondCTE where condition3 ) select column1, count(*) as count from ThirdCTE group by column1 ; DataAnalyticsにおけるCTEs ※ 一般論ではなく、業務を通じて個人的に心掛けたことをまとめる。 ファネルを意識した記述方法 避けたい記述 CTEs書くと記述がばらつく問題 「十人十色、多様性の時代ではあるが記述を統一したいよなぁ〜」と悩み出す。 個人的に気を遣って書いている事をまとめてみる。\nオ◯ニークエリ 意味：自分だけしか解読できないようなクエリのこと 対処： 誰が読んでも分かるような記述を心掛ける。(認知コストを奪うことはしたくない意図) eg. サブクエリのネストは２段まで…etc. 毒親クエリ 意味：過保護すぎるコメント記述でゴチャつくクエリのこと 対処： クエリを読んで理解できる自明に対しては説明を控える。 一方、where pref_flag = \u0026#39;3\u0026#39; --3:福岡県のようなクエリに対してはコメントする。 Ref. CTEs(Common Table Expressions)について学び直す Zenn\nサブクエリと CTE の作成 GoogleCloudSkillBoost\nBigQuery 集計・分析の為の記述構成\n再帰 CTE を使用する GoogleCloud\nBigQuery で WITH RECURSIVE 句が GA になりました！ Zenn\ndbtでのSQLモデル記述時に利用推奨されている『共通テーブル式(CTE/Common Table Expression)』について DevelopersIO\n","date":1721459622,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721459622,"objectID":"001751489e39ebb07db0c15f728b9e20","permalink":"http://localhost:1313/todayilearned/post/bigquery_ctes_in_data_analyisis/","publishdate":"2024-07-20T16:13:42+09:00","relpermalink":"/post/bigquery_ctes_in_data_analyisis/","section":"post","summary":"🔍 Things to keep in mind when analyzing data using SQL.","tags":["BigQuery","SQL","Data Analytics","KnowHow"],"title":"BigQuery CTEs in Data Analyisis","type":"post"},{"authors":null,"categories":[""],"content":" Why SSIA\nWhat Issues とは GitHubのIssues は、様々なシーンで使用される便利なツール。\nプロジェクトのバグ報告 機能リクエスト タスクのトラッキング ディスカッション 正しい使い方を理解することで、プロジェクトの管理やコラボレーションがスムーズになる。 共同開発と個人開発でのユースケースを例を入れてメモする。 共同開発でのIssuesの使い方 Issuesの作成： タイトル: 問題やリクエストの要点を簡潔に表現する。 ラベル: Issueの種類（バグ、機能リクエスト、質問など）や優先度を示すためにラベルを追加する。 アサイン: 担当者を指定する。 説明: 問題の詳細、再現手順、期待される結果、スクリーンショットなどを含める。 例1：バグ報告 タイトル: “アプリが特定の操作でクラッシュする” ラベル: bug, high priority アサイン: @developer1 説明: ### 概要 アプリがユーザーが「設定」メニューを開くとクラッシュします。 ### 再現手順 1. アプリを開く 2. 「設定」メニューをクリックする 3. アプリがクラッシュする ### 期待される結果 設定メニューが正常に表示されること。 ### 実際の結果 アプリがクラッシュし、エラーメッセージが表示される。 ### 環境 - OS: macOS 11.4 - アプリバージョン: 1.0.0 - デバイス: MacBook Pro 2020 例2：機能リクエスト タイトル: “ダークモードのサポートを追加する” ラベル: enhancement, feature request アサイン: @designer1, @developer2 説明: ### 概要 アプリにダークモードを追加することで、夜間や暗い環境での使用時の目の負担を軽減します。 ### 詳細 - ユーザーが「設定」からダークモードを選択できるようにする。 - ダークモードを選択すると、アプリの全てのUIが暗いテーマに変更される。 ### 利点 - 目の負担が軽減される。 - ユーザーエクスペリエンスが向上する。 Issuesの管理： ステータス更新： Issueが進行中の場合は、「In Progress」などのラベルを追加。 進捗状況をコメントで共有。 関連Issueのリンク： 関連するIssueやPull Requestをリンクして、情報を統合。 e.g.) Related to #123（Issue番号123に関連） クローズ： 問題が解決したら、Issueをクローズ。 e.g.) Fixed by #456（Pull Request番号456で解決） 具体例の流れ Issueの作成： ユーザーがアプリのクラッシュを報告するためにIssueを作成。 コメントでのディスカッション： 開発者が再現手順や追加情報を確認するためにコメントを追加。 修正作業： 開発者が修正に取り組み、進捗をコメントで報告。 @developer1が修正完了後にプルリクエスト（PR）を作成。 レビューとマージ： 他の開発者がPRをレビューし、承認。 PRがマージされ、Issueがクローズされる。 まとめ GitHubのIssuesを正しく使うことで、プロジェクトの管理が効率化され、チームメンバー間のコミュニケーションが向上。 明確なタイトル と 詳細な説明 、 適切なラベル とアサインを行うことで、誰が何をするべきかが明確になり、プロジェクトの進行がスムーズになる。 個人開発でのIssuesの使い方 タスク管理： 自分のやるべきタスクをIssueとして登録することで、To-Doリストとして機能する。 各タスクの進捗状況や詳細を追跡できる。 e.g.) タイトル: “Vimの設定を見直す” ラベル: enhancement, task 説明: ### 概要 現在のVim設定を見直し、より効率的な設定に変更する。 ### タスク - プラグインの整理 - キーバインドの見直し - パフォーマンスの向上 ### 期日 2024-08-01 アイデアの記録： 新しい機能や改善点のアイデアを忘れないようにIssueに記録する。 時間があるときに取り組むべきアイデアのリストとして活用する。 e.g.) タイトル: “zshのテーマをカスタマイズする” ラベル: idea, enhancement 説明: ### 概要 zshのテーマを自分好みにカスタマイズすることで、ターミナルの見た目を改善する。 ### アイデア - プロンプトにGitステータスを表示 - 配色を変更 - カスタムプロンプトを追加 バグの追跡： 自分で発見したバグや問題をIssueに記録し、後で修正するためのリマインダーとして使う。 e.g.) タイトル: “dotfilesのシンボリックリンクが正しく作成されない” ラベル: bug, urgent 説明: ### 概要 新しい環境にdotfilesをデプロイする際、シンボリックリンクが正しく作成されない問題が発生。 ### 再現手順 1. リポジトリをクローン 2. セットアップスクリプトを実行 3. シンボリックリンクが正しい場所に作成されていない ### 期待される結果 シンボリックリンクが正しい場所に作成されること。 ### 実際の結果 シンボリックリンクが間違った場所に作成される。 まとめ 個人開発でのIssueの利用は、プロジェクトの進捗管理やアイデアの整理、問題の追跡に非常に役立つ。 定期的にIssuesを見直すことで、自分の作業の進捗や次に取り組むべきタスクが明確になり、効率的な開発が可能になる。 個人プロジェクトであっても、気にする御作法は共同開発の場合と同様で、明確にしてくことを意識すると良い。 Ref. GitHub Issues のドキュメント GitHub Docs Issueについて GitHub Docs 【Github初心者向け】issueの使い方をマスターしよう Zenn githubでissueのテンプレートを作成する方法 Qiita チーム開発を変える「GitHub」とは？〜Issuesの使い方〜【連載第3回】 SELECK GitHubのissueを活用した個人アプリの開発手順を書いてみた Qiita GitHubのIssueでは、Milestoneが役に立つという話 excite GitHubのissueについて Zenn ","date":1721295001,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721295001,"objectID":"42c736033b23b492f1d2afe01eaccf93","permalink":"http://localhost:1313/todayilearned/post/github_how_to_use_issues/","publishdate":"2024-07-18T18:30:01+09:00","relpermalink":"/post/github_how_to_use_issues/","section":"post","summary":"📍 How to effectively use 'Issues' in GitHub repository.","tags":["GitHub","KnowHow"],"title":"GitHub How to use issues","type":"post"},{"authors":null,"categories":[""],"content":"Why What オンラインで無料取得できるデータ 🇯🇵 総務省管理 https://www.stat.go.jp/data/ 🗺️ 地図データ https://www.5656map.jp/ 2D: https://www.5656map.jp/pop_100m.html#11/35.6900/139.6900 3D: https://www.5656map.jp/pop_3d.html#7.97/33.483/130.321/-19.3/30 💰 財務/企業データ https://www.buffett-code.com/ https://www.bloomberg.com/asia egov データポータル https://data.e-gov.go.jp/info/ja i.e. 日本政府が公開している公共データ\u0026amp;調査データを検索できる。（人口、運輸、観光…etc.） UC Irvine Machine Learning Repository https://archive.ics.uci.edu/ i.e. カリフォルニア大学アーバイン校が公開しているデータセット（約400件） e.g. Irisデータセット Google Dataset Search https://datasetsearch.research.google.com/ i.e. Google社が提供しているデータセットを検索できる。 Kaggle https://www.kaggle.com/datasets i.e. 世界的なデータコンペティションで用いられたデータセット。 arXivTime datasets https://github.com/arXivTimes/arXivTimes/tree/master/datasets i.e. 画像、動画、音声、テキストなど種々様々なデータセット。 Awesome Public Datasets https://github.com/awesomedata/awesome-public-datasets#image-processing i.e. 農業、化学、教育…etc. データの品質 日本ファクトチェックセンター (JFC) https://www.factcheckcenter.jp/ Ref. 【市場調査】無料で使える統計データサイト11選 GMO Research\u0026amp;AI データの信頼性とは IBM 組織が発行するデータの信頼性を確保する制度に関する検討会 総務省 ","date":1721225684,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721225684,"objectID":"8982451cda6b5ca8e9cf0065f166b8e8","permalink":"http://localhost:1313/todayilearned/post/stats_free_data_at_onlinei/","publishdate":"2024-07-17T23:14:44+09:00","relpermalink":"/post/stats_free_data_at_onlinei/","section":"post","summary":"📊 An article summarizing various statistical data that can be viewed and used for free.","tags":["Statistic","Data Analytics"],"title":"Static free data at online","type":"post"},{"authors":null,"categories":[],"content":"Hello World ","date":1720337821,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720337821,"objectID":"861a5a1955af9e733c6c2ea967ef443d","permalink":"http://localhost:1313/todayilearned/post/my_first_post/","publishdate":"2024-07-07T16:37:01+09:00","relpermalink":"/post/my_first_post/","section":"post","summary":"A brief summary of the article.","tags":[],"title":"My First Post","type":"post"}]